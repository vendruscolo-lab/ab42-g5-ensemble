{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:20:31.445882Z",
     "start_time": "2021-07-28T17:20:31.440146Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill params\n",
    "ratio = 0.9          # Train-Test split ratio\n",
    "attempt = 0        # Number of times to run\n",
    "width = 512\n",
    "depth = 2\n",
    "learning_rate = 1e-2\n",
    "dropout = 0.05\n",
    "regularization = 1e-7\n",
    "epsilon = 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "In this notebook we set up the neural networks with VAMPNet scoring functions and train them for all ensembles with different output sizes and estimate errors by bootstrap aggregation. This notebook can be used with `papermill` to run all cells automatically with given parameters. We first define the imports and useful utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:20:41.427321Z",
     "start_time": "2021-07-28T17:20:35.971616Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/tf/lib/python3.5/site-packages/pyemma/__init__.py:130: UserWarning: You are not using the latest release of PyEMMA. Latest is 2.5.7, you have 2.5.4.\n",
      "  .format(latest=latest, current=current), category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%run model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:20:41.432553Z",
     "start_time": "2021-07-28T17:20:41.428900Z"
    }
   },
   "outputs": [],
   "source": [
    "def statdist(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the equilibrium distribution of a transition matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        Row-stochastic transition matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mu\n",
    "        Stationary distribution, i.e. the left\n",
    "        eigenvector associated with eigenvalue 1.\n",
    "    \n",
    "    \"\"\"\n",
    "    ev, evec = eig(X, left=True, right=False)\n",
    "    mu = evec.T[ev.argmax()]\n",
    "    mu /= mu.sum()\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:20:41.483156Z",
     "start_time": "2021-07-28T17:20:41.433878Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_split(data: MaybeListType[np.ndarray], lag: int, p: float=0.1, mask: Optional[np.ndarray]=None):\n",
    "    data = make_list(data)\n",
    "    lengths = np.array([len(d) for d in data])\n",
    "    nframes = lengths.sum()\n",
    "    \n",
    "    inds = np.empty((nframes, 4), dtype=np.int)\n",
    "    inds[:, 0] = np.repeat(np.arange(len(data), dtype=np.int), lengths)\n",
    "    inds[:, 1] = np.concatenate([np.arange(n) for n in lengths])\n",
    "    inds[:, 2] = np.arange(nframes, dtype=np.int)\n",
    "    inds[:, 3] = np.zeros_like(inds[:, 0]) if mask is None else mask\n",
    "    inds = unflatten(inds, lengths=[lengths])\n",
    "    \n",
    "    # Local (frame) shuffling\n",
    "    shuf_traj_inds = [np.random.choice(\n",
    "        d[:, 1], size=d.shape[FRAMES], replace=False) for d in inds]\n",
    "    \n",
    "    # Sort out too short trajectories, split out lagged part\n",
    "    n_pairs = 0\n",
    "    xt, xttau = [], []\n",
    "    for i, traj in enumerate(inds):\n",
    "        n_points = traj.shape[FRAMES]\n",
    "\n",
    "        # We'll just skip super short trajectories for now\n",
    "        shuf_traj_inds[i] = shuf_traj_inds[i][shuf_traj_inds[i] < (n_points - lag)]\n",
    "        if n_points <= lag:\n",
    "            continue\n",
    "                \n",
    "        n_pairs += n_points - lag\n",
    "        xt.append(traj[:n_points - lag][shuf_traj_inds[i]])\n",
    "        xttau.append(traj[lag:n_points][shuf_traj_inds[i]])\n",
    "        \n",
    "    # Shuffle externally\n",
    "    shuf_full_inds = np.random.choice(\n",
    "        np.arange(n_pairs, dtype=np.int), size=n_pairs, replace=False)\n",
    "    xt_shuf = np.vstack(xt)[shuf_full_inds]\n",
    "    xttau_shuf = np.vstack(xttau)[shuf_full_inds]\n",
    "    \n",
    "    # These are the entries for the test set\n",
    "    n_frames_test = int(xt_shuf.shape[FRAMES] * p)\n",
    "    inds_t = xt_shuf[:n_frames_test]\n",
    "    inds_ttau = xttau_shuf[:n_frames_test]\n",
    "    data_flat = np.vstack(data)\n",
    "    test_xt, test_xttau = data_flat[inds_t[:, 2]], data_flat[inds_ttau[:, 2]]\n",
    "    \n",
    "    # Mask out unwanted frames with NaNs\n",
    "    mask_pair = xt_shuf[:, 3] | xttau_shuf[:, 3]\n",
    "    test_xt[mask_pair[:n_frames_test]] = np.nan\n",
    "    test_xttau[mask_pair[:n_frames_test]] = np.nan\n",
    "    \n",
    "    # We can't just remove our test frame pairs, as the training set\n",
    "    # would then be out of sync! So we replace the test samples with\n",
    "    # NaNs instead, we can check for those later in the DataGenerator.\n",
    "    data_flat[np.union1d(inds_t[:, 2], inds_ttau[:, 2])] = np.nan\n",
    "    data_train_valid = unflatten(data_flat, lengths=[lengths])\n",
    "        \n",
    "    return data_train_valid, (test_xt, test_xttau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in multiple rounds of 1024 simulations each at 278 K in the $NVT$ ensemble yielding approximately 300 µs per ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:20:43.404724Z",
     "start_time": "2021-07-28T17:20:43.365305Z"
    }
   },
   "outputs": [],
   "source": [
    "sim_names = (\"apo\", \"holo\", \"control\")\n",
    "top, trajs = {}, {}\n",
    "trajs = {k: sorted(glob(\"trajectories/{0}/r?/traj*.xtc\".format(k))) for k in sim_names}\n",
    "top = {k: \"trajectories/{0}/topol.gro\".format(k) for k in sim_names}\n",
    "KBT = 2.311420 # 278 K\n",
    "nres = 42\n",
    "traj_rounds = {\n",
    "    \"apo\": [1024, 1023, 1024, 1024, 1024],\n",
    "    \"holo\": [1023, 1024, 32],\n",
    "    \"control\": [1024, 1023]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:21:08.335722Z",
     "start_time": "2021-07-28T17:20:47.916222Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "allpairs = np.asarray(list(itertools.combinations(range(nres), 2)))\n",
    "inpcon = {}\n",
    "for k in sim_names:\n",
    "    feat = pe.coordinates.featurizer(top[k])\n",
    "    feat.add_residue_mindist(residue_pairs=allpairs)\n",
    "    inpcon[k] = pe.coordinates.source(trajs[k], feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:21:08.365084Z",
     "start_time": "2021-07-28T17:21:08.337287Z"
    }
   },
   "outputs": [],
   "source": [
    "lengths, nframes = {}, {}\n",
    "for i, k in enumerate(sim_names):\n",
    "    lengths[k] = sort_lengths(inpcon[k].trajectory_lengths(), traj_rounds[k])\n",
    "    nframes[k] = inpcon[k].trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:21:08.382569Z",
     "start_time": "2021-07-28T17:21:08.366721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tapo\t\tholo\t\tcontrol\t\tphen\n",
      "Trajs: \t\t5119\t\t2079\t\t2047\t\t2048\n",
      "Frames: \t1259172\t\t1225868\t\t1114503\t\t1236792\n",
      "Time: \t\t314.793 µs\t306.467 µs\t278.626 µs\t309.198 µs\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\t\" + \"\\t\\t\".join(sim_names))\n",
    "print(\"\\n\".join((\n",
    "    \"Trajs: \\t\\t\" + \"\\t\\t\".join(\"{0}\".format(len(trajs[k])) for k in sim_names),\n",
    "    \"Frames: \\t\" + \"\\t\\t\".join(\"{0}\".format(nframes[k]) for k in sim_names),\n",
    "    \"Time: \\t\\t\" + \"\\t\".join(\"{0:5.3f} µs\".format(inpcon[k].trajectory_lengths().sum() * 0.00025)\n",
    "                           for k in sim_names)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAMPNet\n",
    "VAMPNet[1] is composed of two lobes, one reading the system features $\\mathbf{x}$ at a timepoint $t$ and the other after some lag time $\\tau$. In this case the network reads all minimum inter-residue distances (780 values) and sends them through 5 layers with 256 nodes each. The final layer uses between 2 and 8 *softmax* outputs to yield a state assignment vector $\\chi: \\mathbb{R}^m \\to \\Delta^{n}$ where $\\Delta^{n} = \\{ s \\in \\mathbb{R}^n \\mid 0 \\le s_i \\le 1, \\sum_i^n s_i = 1 \\}$ representing the probability of a state assignment. One lobe thus transforms a system state into a state occupation probability. We can also view this value as a kind of reverse ambiguity, i.e. how sure the network is that the system is part of a certain cluster. These outputs are then used as the input for the VAMP scoring function. We use the new enhanced version with physical constraints[2], particularly the ones for positive entries and reversibility.\n",
    "\n",
    "[1] Mardt, A., Pasquali, L., Wu, H. & Noé, F. VAMPnets for deep learning of molecular kinetics. Nat Comms 1–11 (2017). doi:10.1038/s41467-017-02388-1\n",
    "\n",
    "[2] Mardt, A., Pasquali, L., Noé, F. & Wu, H. Deep learning Markov and Koopman models with physical constraints. arXiv:1912.07392 [physics] (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) and first normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:21:08.387370Z",
     "start_time": "2021-07-28T17:21:08.384088Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in sim_names:\n",
    "    filename = \"intermediate/mindist-all-{0}.npy\".format(k)\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"No mindist file for {0} ensemble, calculating from scratch...\".format(k))\n",
    "        con = np.vstack(inpcon[k].get_output())\n",
    "        np.save(filename, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:22:53.980295Z",
     "start_time": "2021-07-28T17:21:08.388789Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = np.triu_indices(nres, k=1)\n",
    "mat = np.zeros((nres, nres), dtype=np.int)\n",
    "full_flat, full_data = {}, {}\n",
    "for k in sim_names:\n",
    "    raw = np.load(\"intermediate/mindist-all-{0}.npy\".format(k))\n",
    "    mat[idx] = np.arange(raw.shape[1])\n",
    "    redinds = mat[np.triu_indices_from(mat, k=3)]\n",
    "    full_flat[k] = ((raw - raw.mean(axis=0)) / raw.std(axis=0))[:, redinds]\n",
    "    full_data[k] = unflatten(full_flat[k], lengths[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters\n",
    "To allow for a larger hyperparameter search space, we use the self-normalizing neural network approach by Klambauer *et al.* [2], thus using SELU units, `AlphaDropout` and normalized `LeCun` weight initialization. The other hyperparameters are defined at the beginning of this notebook.\n",
    "\n",
    "[2] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-Normalizing Neural Networks. arXiv.org cs.LG, (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:22:53.988021Z",
     "start_time": "2021-07-28T17:22:53.982016Z"
    }
   },
   "outputs": [],
   "source": [
    "activation = \"selu\"                 # NN activation function\n",
    "init = \"lecun_normal\"               # NN weight initialization\n",
    "lag = 20                            # Lag time\n",
    "n_epoch = 100                       # Max. number of epochs\n",
    "n_epoch_s = 10000                   # Max. number of epochs for S optimization\n",
    "n_batch = 10000                     # Training batch size\n",
    "n_dims = full_data[k][0].shape[1]   # Input dimension\n",
    "nres = 42                           # Number of residues\n",
    "epsilon = 1e-7                      # Floating point noise\n",
    "dt = 0.25                           # Trajectory timestep in ns\n",
    "steps = 6                           # CK test steps\n",
    "bs_frames = 1000000                 # Number of frames in the bootstrap sample\n",
    "n_tries = 1                         # Number of training attempts for each model, we pick the best scoring one\n",
    "\n",
    "outsizes = np.array([4, 2, 3, 5, 6])\n",
    "lags = np.array([1, 2, 5, 10, 20, 50, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bound ensemble\n",
    "We filter out unbound frames to avoid issues with potential non-equilibrium conditions, as we see essentially no unbinding events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:22:58.578374Z",
     "start_time": "2021-07-28T17:22:53.989404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "inter_pairs = list(itertools.product(range(nres), (nres,)))\n",
    "intercon = {}\n",
    "for k in (\"holo\", \"control\"):\n",
    "    feat = pe.coordinates.featurizer(top[k])\n",
    "    feat.add_residue_mindist(residue_pairs=inter_pairs)\n",
    "    intercon[k] = pe.coordinates.source(trajs[k], feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:23:00.891675Z",
     "start_time": "2021-07-28T17:22:58.579969Z"
    }
   },
   "outputs": [],
   "source": [
    "inter_con = {}\n",
    "for k in (\"holo\", \"control\"):\n",
    "    filename = \"intermediate/mindist-inter-all-{0}.npy\".format(k)\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"No mindist file for {0} ensemble, calculating from scratch...\".format(k))\n",
    "        inter_con[k] = np.vstack(intercon[k].get_output())\n",
    "        np.save(filename, inter_con)\n",
    "    else:\n",
    "        inter_con[k] = np.load(filename)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:54:08.417158Z",
     "start_time": "2021-03-25T23:54:08.405339Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:55:26.882431Z",
     "start_time": "2021-03-25T23:54:08.939483Z"
    }
   },
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "for i, k in enumerate((\"holo\", \"control\")):\n",
    "    x = np.arange(0.0, 4.0, 0.01)\n",
    "    y = np.array([(inter_con[k] < c).any(axis=1).sum() / inter_con[k].shape[0] for c in x])\n",
    "    ax.plot(x, y, linewidth=2.5, color=colors[i + 4], label=k)\n",
    "ax.tick_params(labelsize=24)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.set_xlabel(\"Cutoff [nm]\", fontsize=24)\n",
    "ax.set_ylabel(\"Proportion\", fontsize=24)\n",
    "ax.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:32:06.351531Z",
     "start_time": "2021-07-28T17:32:06.218700Z"
    }
   },
   "outputs": [],
   "source": [
    "cutoff = 0.5  # Compromise given the above plot\n",
    "masks = {k: (inter_con[k] < cutoff).any(axis=1) for k in (\"holo\",)}\n",
    "masks.update({k: np.full(full_flat[k].shape[0], True) for k in (\"apo\", \"control\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T17:41:44.836973Z",
     "start_time": "2021-07-28T17:41:10.545014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading existing input file for holo ensemble...\n",
      "Reading existing input file for phen ensemble...\n",
      "Reading existing input file for apo ensemble...\n",
      "Reading existing input file for control ensemble...\n"
     ]
    }
   ],
   "source": [
    "input_data, input_flat, test_data = {}, {}, {}\n",
    "for k in (\"holo\", \"apo\", \"control\"):\n",
    "    filename = \"intermediate/input-mask-{0}.npz\".format(k)\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"No input file for {0} ensemble, resplitting...\".format(k))\n",
    "        input_data[k], test_data[k] = test_split(full_data[k], lag=lag, mask=~masks[k])\n",
    "        input_flat[k] = np.vstack(input_data[k])\n",
    "        input_flat[k][~masks[k]] = np.nan\n",
    "        np.savez(filename, data=input_flat[k], test_t=test_data[k][0], test_ttau=test_data[k][1])\n",
    "    else:\n",
    "        print(\"Reading existing input file for {0} ensemble...\".format(k))\n",
    "        raw = np.load(filename)\n",
    "        input_flat[k], test_data[k] = raw[\"data\"], (raw[\"test_t\"], raw[\"test_ttau\"])\n",
    "        input_data[k] = unflatten(input_flat[k], lengths=lengths[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "We run the training several times with different train/test splits to get an error estimate, this is referred to as bootstrap aggregating (*bagging*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T00:42:40.453470Z",
     "start_time": "2020-11-07T00:42:32.629640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training holo n=3 i=1...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b6f1345992a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     batchnorm=False, lr_factor=5e-3))\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mkoop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkoop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mkoops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/notebooks/holo-ensemble/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, generator)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m         \"\"\"\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         self._models = _build_model(self.n_input, self.n_output,\n\u001b[1;32m   1779\u001b[0m                                     verbose=self.verbose, **self.nnargs)\n",
      "\u001b[0;32m/home/thomas/notebooks/holo-ensemble/model.py\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(self, generator)\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_lag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m             self._models = _build_model(self.n_input, self.n_output,\n",
      "\u001b[0;32m/home/thomas/notebooks/holo-ensemble/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, n, lag)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mxttau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mxt_shuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mxttau_shuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxttau\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;31m# Collect all selected indices for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with h5py.File(\"intermediate/data.hdf5\", \"a\") as write:\n",
    "    for k in (\"holo\",):\n",
    "        \n",
    "        # Create HDF5 groups\n",
    "        ens = write.require_group(k)\n",
    "        att = ens.require_group(str(attempt))\n",
    "        \n",
    "        # Generate or read previously generated\n",
    "        index_file = \"models/model-idx-{0}-{1}.hdf5\".format(k, attempt)\n",
    "        if os.path.exists(index_file):\n",
    "            generator = DataGenerator.from_state(input_data[k], index_file)\n",
    "        else:\n",
    "            generator = DataGenerator(input_data[k], dt=dt)\n",
    "            generator.save(index_file)\n",
    "        \n",
    "        for n in outsizes:\n",
    "            print(\"Training {0} n={1} i={2}...\".format(k, n, attempt + 1))\n",
    "            out = att.require_group(str(n))\n",
    "            \n",
    "            tests = test_data[k], np.zeros((test_data[k][0].shape[0], 2 * n))\n",
    "            koops, scores = [], np.empty(n_tries)\n",
    "            for i in range(n_tries):\n",
    "                koop = KoopmanModel(n=n, network_lag=lag, verbose=1, nnargs=dict(\n",
    "                    width=width, depth=depth, learning_rate=learning_rate,\n",
    "                    regularization=regularization, dropout=dropout,\n",
    "                    batchnorm=False, lr_factor=2e-2))\n",
    "                koop.fit(generator)\n",
    "                scores[i] = koop.score(tests)\n",
    "                koops.append(koop)\n",
    "                \n",
    "            koop = koops[scores.argmax()]\n",
    "            koop.save(\"models/model-ve-{0}-{1}-{2}.hdf5\".format(k, n, attempt))\n",
    "            print(\"Estimating Koopman operator...\")\n",
    "            ko = out.require_dataset(\"k\", shape=(n, n), dtype=\"float32\")\n",
    "            ko[:] = koop.estimate_koopman(lag=50)\n",
    "            print(\"Estimating mu...\")\n",
    "            mu = out.require_dataset(\"mu\", shape=(koop.data.n_train,), dtype=\"float32\")\n",
    "            mu[:] = koop.mu\n",
    "            print(\"Estimating implied timescales...\")\n",
    "            its = out.require_dataset(\"its\", shape=(n - 1, len(lags)), dtype=\"float32\")\n",
    "            its[:] = koop.its(lags)\n",
    "            print(\"Performing CK-test...\")\n",
    "            cke = out.require_dataset(\"cke\", shape=(n, n, steps), dtype=\"float32\")\n",
    "            ckp = out.require_dataset(\"ckp\", shape=(n, n, steps), dtype=\"float32\")\n",
    "            cke[:], ckp[:] = koop.cktest(steps)\n",
    "            print(\"Estimating chi...\")\n",
    "            bootstrap = out.require_dataset(\"bootstrap\", shape=(koop.data.n_train, 2 * n), dtype=\"float32\")\n",
    "            bootstrap[:] = koop.transform(koop.data.trains[0])\n",
    "            full = out.require_dataset(\"full\", shape=(nframes[k], 2 * n), dtype=\"float32\")\n",
    "            full[:] = koop.transform(generator.data_flat)\n",
    "            del koop, koops\n",
    "            gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
