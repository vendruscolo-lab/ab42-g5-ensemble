{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:12.179847Z",
     "start_time": "2020-11-23T15:07:12.173370Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill params\n",
    "ratio = 0.9          # Train-Test split ratio\n",
    "n_iter = 100         # Number of times to run\n",
    "epsilon = 1e-7\n",
    "n_runs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "In this notebook we set up the neural networks with VAMPNet scoring functions and train them for different output sizes and estimate errors by bootstrap aggregation. This notebook can be used with `papermill` to run all cells automatically with given parameters. We first define the imports and useful utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T20:43:09.074886Z",
     "start_time": "2020-11-23T20:43:09.053297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:18.960595Z",
     "start_time": "2020-11-23T15:07:18.955997Z"
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "def statdist(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the equilibrium distribution of a transition matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        Row-stochastic transition matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mu\n",
    "        Stationary distribution, i.e. the left\n",
    "        eigenvector associated with eigenvalue 1.\n",
    "    \n",
    "    \"\"\"\n",
    "    ev, evec = eig(X, left=True, right=False)\n",
    "    mu = evec.T[ev.argmax()]\n",
    "    mu /= mu.sum()\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:19.009020Z",
     "start_time": "2020-11-23T15:07:18.962398Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_split(data: MaybeListType[np.ndarray], lag: int, p: float=0.1):\n",
    "    data = make_list(data)\n",
    "    lengths = np.array([len(d) for d in data])\n",
    "    nframes = lengths.sum()\n",
    "    \n",
    "    inds = np.empty((nframes, 3), dtype=np.int)\n",
    "    inds[:, 0] = np.repeat(np.arange(len(data), dtype=np.int), lengths)\n",
    "    inds[:, 1] = np.concatenate([np.arange(n) for n in lengths])\n",
    "    inds[:, 2] = np.arange(nframes, dtype=np.int)\n",
    "    inds = unflatten(inds, lengths=[lengths])\n",
    "    \n",
    "    # Local (frame) shuffling\n",
    "    shuf_traj_inds = [np.random.choice(\n",
    "        d[:, 1], size=d.shape[FRAMES], replace=False) for d in inds]\n",
    "    \n",
    "    # Sort out too short trajectories, split out lagged part\n",
    "    n_pairs = 0\n",
    "    xt, xttau = [], []\n",
    "    for i, traj in enumerate(inds):\n",
    "        n_points = traj.shape[FRAMES]\n",
    "\n",
    "        # We'll just skip super short trajectories for now\n",
    "        shuf_traj_inds[i] = shuf_traj_inds[i][shuf_traj_inds[i] < (n_points - lag)]\n",
    "        if n_points <= lag:\n",
    "            continue\n",
    "                \n",
    "        n_pairs += n_points - lag\n",
    "        xt.append(traj[:n_points - lag][shuf_traj_inds[i]])\n",
    "        xttau.append(traj[lag:n_points][shuf_traj_inds[i]])\n",
    "        \n",
    "    # Shuffle externally\n",
    "    shuf_full_inds = np.random.choice(\n",
    "        np.arange(n_pairs, dtype=np.int), size=n_pairs, replace=False)\n",
    "    xt_shuf = np.vstack(xt)[shuf_full_inds]\n",
    "    xttau_shuf = np.vstack(xttau)[shuf_full_inds]\n",
    "    \n",
    "    # These are the entries for the test set\n",
    "    inds_t = xt_shuf[:int(xt_shuf.shape[FRAMES] * p)]\n",
    "    inds_ttau = xttau_shuf[:int(xt_shuf.shape[FRAMES] * p)]\n",
    "    data_flat = np.vstack(data)\n",
    "    test_xt, test_xttau = data_flat[inds_t[:, 2]], data_flat[inds_ttau[:, 2]]\n",
    "    \n",
    "    # We can't just remove our test frame pairs, as the training set\n",
    "    # would then be out of sync! So we replace the test samples with\n",
    "    # NaNs instead, we can check for those later in the DataGenerator.\n",
    "    data_flat[np.union1d(inds_t[:, 2], inds_ttau[:, 2])] = np.nan\n",
    "    data_train_valid = unflatten(data_flat, lengths=[lengths])\n",
    "        \n",
    "    return data_train_valid, (test_xt, test_xttau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:19.178385Z",
     "start_time": "2020-11-23T15:07:19.010841Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, ticker\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "# Plot settings\n",
    "sns.set_palette(\"husl\", 8)\n",
    "rc(\"font\", **{\"family\": \"Helvetica\",\n",
    "              \"sans-serif\": [\"Helvetica\"]})\n",
    "rc(\"svg\", **{\"fonttype\": \"none\"})\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:20.090522Z",
     "start_time": "2020-11-23T15:07:20.067148Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_its(its, lags, dt=1.0):\n",
    "    multi = its.ndim == 3\n",
    "    nits, nlags = its.shape[-2], its.shape[-1]\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    if multi:\n",
    "        itsm = its.mean(axis=0)\n",
    "        cfl, cfu = np.percentile(its, q=(2.5, 97.5), axis=0)\n",
    "    else:\n",
    "        itsm = its\n",
    "    \n",
    "    ax.semilogy(lags * dt, lags * dt, color=\"k\")\n",
    "    ax.fill_between(lags * dt, ax.get_ylim()[0] * np.ones(len(lags)),\n",
    "                    lags * dt, color=\"k\", alpha=0.2)\n",
    "    for i in range(nits):\n",
    "        ax.plot(lags * dt, itsm[i], marker=\"o\",\n",
    "                    linestyle=\"dashed\", linewidth=1.5, color=colors[-(i + 2)])\n",
    "        ax.plot(lags * dt, itsm[i], marker=\"o\", linewidth=1.5, color=colors[-(i + 2)])\n",
    "        if multi:\n",
    "            ax.fill_between(lags * dt, cfl[i], cfu[i],\n",
    "                            interpolate=True, color=colors[-(i + 2)], alpha=0.2)\n",
    "    loc = ticker.LogLocator(base=10.0, subs=(0.2, 0.4, 0.6, 0.8), numticks=12)\n",
    "    ax.set_ylim(1, 5000)\n",
    "    ax.set_yticks(10 ** np.arange(5))\n",
    "    ax.yaxis.set_minor_locator(loc)\n",
    "    ax.yaxis.set_minor_formatter(ticker.NullFormatter())\n",
    "    ax.set_xlabel(r\"$\\tau$ [ns]\", fontsize=24)\n",
    "    ax.set_ylabel(r\"$t_i$ [ns]\", fontsize=24)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    sns.despine(ax=ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:25.984247Z",
     "start_time": "2020-11-23T15:07:25.950585Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_ck(cke, ckp, lag):\n",
    "    multi = cke.ndim == 4\n",
    "    n = cke.shape[-2]\n",
    "    steps = cke.shape[-1]\n",
    "    \n",
    "    if multi:\n",
    "        ckem = cke.mean(axis=0)\n",
    "        ckpm = ckp.mean(axis=0)\n",
    "        ckep = np.percentile(cke, q=(2.5, 97.5), axis=0)\n",
    "        ckpp = np.percentile(ckp, q=(2.5, 97.5), axis=0)\n",
    "    else:\n",
    "        ckem = cke\n",
    "        ckpm = ckp\n",
    "    \n",
    "    fig, axes = plt.subplots(n, n, figsize=(4 * n, 4 * n), sharex=True)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax = axes[i, j]\n",
    "            x = np.arange(0, steps * lag, lag)\n",
    "            if multi:\n",
    "                ax.errorbar(x, ckpm[i, j], yerr=[ckpm[i, j] - ckpp[0, i, j], ckpp[1, i, j] - ckpm[i, j]],\n",
    "                            linewidth=2, elinewidth=2)\n",
    "                ax.fill_between(x, ckep[0, i, j], ckep[1, i, j],\n",
    "                                alpha=0.2, interpolate=True, color=colors[1])\n",
    "            else:\n",
    "                ax.plot(x, ckpm[i, j], linestyle=\"-\", color=colors[0], linewidth=2)\n",
    "            ax.plot(x, ckem[i, j], linestyle=\"--\", color=colors[1], linewidth=2)\n",
    "            \n",
    "            if i == j:\n",
    "                ax.set_ylim(0.78, 1.02)\n",
    "                ax.text(0, 0.8, r\"{0} $\\to$ {1}\".format(i, j), fontsize=24, verticalalignment=\"center\")\n",
    "            else:\n",
    "                ax.set_ylim(-0.02, 0.22)\n",
    "                ax.text(0, 0.2, r\"{0} $\\to$ {1}\".format(i, j), fontsize=24, verticalalignment=\"center\")\n",
    "            ax.set_xticks(np.arange(0, steps * lag, lag), minor=True)\n",
    "            ax.set_xticks(np.arange(0, steps * lag, 2 * lag))\n",
    "            ax.set_xticklabels((np.arange(0, steps * lag, 2 * lag) * dt).astype(int))\n",
    "            ax.tick_params(labelsize=24)\n",
    "    fig.text(0.5, 0.01 * 1.5 * n, r\"$\\tau$ [ns]\", ha=\"center\", fontsize=24)\n",
    "    fig.text(0.01 * 1.5 * n, 0.5, r\"$P$\", va=\"center\", rotation=\"vertical\", fontsize=24)\n",
    "    fig.subplots_adjust(wspace=0.25)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in multiple rounds of 1024 simulations each at 278 K in the $NVT$ ensemble yielding approximately 300 µs per ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:48.547830Z",
     "start_time": "2020-11-23T15:07:48.474467Z"
    }
   },
   "outputs": [],
   "source": [
    "sim_names = (\"apo\", \"holo\", \"control\")\n",
    "top, trajs = {}, {}\n",
    "trajs = {k: sorted(glob(\"trajectories/{0}/r?/traj*.xtc\".format(k))) for k in sim_names}\n",
    "top = {k: \"trajectories/{0}/topol.gro\".format(k) for k in sim_names}\n",
    "KBT = 2.311420 # 278 K\n",
    "nres = 42\n",
    "traj_rounds = {\n",
    "    \"apo\": [1024, 1023, 1024, 1024, 1024],\n",
    "    \"holo\": [1023, 1024, 32],\n",
    "    \"control\": [1024, 1023]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:44.514059Z",
     "start_time": "2020-11-23T15:07:59.292453Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Obtaining file info'),), layout=Layout(max_width='35%', min_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "allpairs = np.asarray(list(itertools.combinations(range(nres), 2)))\n",
    "inpcon = {}\n",
    "for k in sim_names:\n",
    "    feat = pe.coordinates.featurizer(top[k])\n",
    "    feat.add_residue_mindist(residue_pairs=allpairs)\n",
    "    inpcon[k] = pe.coordinates.source(trajs[k], feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:44.547948Z",
     "start_time": "2020-11-23T15:08:44.516012Z"
    }
   },
   "outputs": [],
   "source": [
    "lengths, nframes = {}, {}\n",
    "for i, k in enumerate(sim_names):\n",
    "    lengths[k] = sort_lengths(inpcon[k].trajectory_lengths(), traj_rounds[k])\n",
    "    nframes[k] = inpcon[k].trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:44.566860Z",
     "start_time": "2020-11-23T15:08:44.549672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tapo\t\tholo\t\tcontrol\t\tphen\n",
      "Trajs: \t\t5119\t\t2079\t\t2047\t\t2048\n",
      "Frames: \t1259172\t\t1225868\t\t1114503\t\t1236792\n",
      "Time: \t\t314.793 µs\t306.467 µs\t278.626 µs\t309.198 µs\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\t\" + \"\\t\\t\".join(sim_names))\n",
    "print(\"\\n\".join((\n",
    "    \"Trajs: \\t\\t\" + \"\\t\\t\".join(\"{0}\".format(len(trajs[k])) for k in sim_names),\n",
    "    \"Frames: \\t\" + \"\\t\\t\".join(\"{0}\".format(nframes[k]) for k in sim_names),\n",
    "    \"Time: \\t\\t\" + \"\\t\".join(\"{0:5.3f} µs\".format(inpcon[k].trajectory_lengths().sum() * 0.00025)\n",
    "                           for k in sim_names)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAMPNet\n",
    "VAMPNet[1] is composed of two lobes, one reading the system features $\\mathbf{x}$ at a timepoint $t$ and the other after some lag time $\\tau$. In this case the network reads all minimum inter-residue distances (780 values) and sends them through 5 layers with 256 nodes each. The final layer uses between 2 and 8 *softmax* outputs to yield a state assignment vector $\\chi: \\mathbb{R}^m \\to \\Delta^{n}$ where $\\Delta^{n} = \\{ s \\in \\mathbb{R}^n \\mid 0 \\le s_i \\le 1, \\sum_i^n s_i = 1 \\}$ representing the probability of a state assignment. One lobe thus transforms a system state into a state occupation probability. We can also view this value as a kind of reverse ambiguity, i.e. how sure the network is that the system is part of a certain cluster. These outputs are then used as the input for the VAMP scoring function. We use the new enhanced version with physical constraints[2], particularly the ones for positive entries and reversibility.\n",
    "\n",
    "[1] Mardt, A., Pasquali, L., Wu, H. & Noé, F. VAMPnets for deep learning of molecular kinetics. Nat Comms 1–11 (2017). doi:10.1038/s41467-017-02388-1\n",
    "\n",
    "[2] Mardt, A., Pasquali, L., Noé, F. & Wu, H. Deep learning Markov and Koopman models with physical constraints. arXiv:1912.07392 [physics] (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) and first normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:09:04.120458Z",
     "start_time": "2020-11-23T15:09:04.116478Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in sim_names:\n",
    "    filename = \"intermediate/mindist-all-{0}.npy\".format(k)\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"No mindist file for {0} ensemble, calculating from scratch...\".format(k))\n",
    "        con = np.vstack(inpcon[k].get_output())\n",
    "        np.save(filename, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:11:14.669516Z",
     "start_time": "2020-11-23T15:09:05.131272Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = np.triu_indices(nres, k=1)\n",
    "mat = np.zeros((nres, nres), dtype=np.int)\n",
    "full_flat, full_data = {}, {}\n",
    "for k in sim_names:\n",
    "    raw = np.load(\"intermediate/mindist-all-{0}.npy\".format(k))\n",
    "    mat[idx] = np.arange(raw.shape[1])\n",
    "    redinds = mat[np.triu_indices_from(mat, k=3)]\n",
    "    full_flat[k] = ((raw - raw.mean(axis=0)) / raw.std(axis=0))[:, redinds]\n",
    "    full_data[k] = unflatten(full_flat[k], lengths[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters\n",
    "To allow for a larger hyperparameter search space, we use the self-normalizing neural network approach by Klambauer *et al.* [2], thus using SELU units, `AlphaDropout` and normalized `LeCun` weight initialization. The other hyperparameters are defined at the beginning of this notebook.\n",
    "\n",
    "[2] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-Normalizing Neural Networks. arXiv.org cs.LG, (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T20:49:41.987710Z",
     "start_time": "2020-11-23T20:49:41.980077Z"
    }
   },
   "outputs": [],
   "source": [
    "activation = \"selu\"                 # NN activation function\n",
    "init = \"lecun_normal\"               # NN weight initialization\n",
    "lag = 20                            # Lag time\n",
    "n_epoch = 100                       # Max. number of epochs\n",
    "n_epoch_s = 10000                   # Max. number of epochs for S optimization\n",
    "n_batch = 10000                     # Training batch size\n",
    "n_dims = full_data[k][0].shape[1]  # Input dimension\n",
    "nres = 42                           # Number of residues\n",
    "epsilon = 1e-7                      # Floating point noise\n",
    "dt = 0.25                           # Trajectory timestep in ns\n",
    "steps = 6                           # CK test steps\n",
    "bs_frames = 1000000                 # Number of frames in the bootstrap sample\n",
    "n_tries = 3                         # Number of training attempts for each model, we pick the best scoring one\n",
    "\n",
    "outsizes = np.array([4, 2, 3, 5, 6])\n",
    "lags = np.array([1, 2, 5, 10, 20, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T20:15:10.980804Z",
     "start_time": "2020-11-23T20:14:17.975729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading existing input file for holo ensemble...\n",
      "Reading existing input file for control ensemble...\n",
      "Reading existing input file for phen ensemble...\n"
     ]
    }
   ],
   "source": [
    "input_data, input_flat, test_data = {}, {}, {}\n",
    "for k in (\"holo\", \"control\"):\n",
    "    filename = \"intermediate/input-{0}.npz\".format(k)\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"No input file for {0} ensemble, resplitting...\".format(k))\n",
    "        input_data[k], test_data[k] = test_split(full_data[k], lag=lag)\n",
    "        input_flat[k] = np.vstack(input_data[k])\n",
    "        np.savez(filename, data=input_flat[k], test_t=test_data[k][0], test_ttau=test_data[k][1])\n",
    "    else:\n",
    "        print(\"Reading existing input file for {0} ensemble...\".format(k))\n",
    "        raw = np.load(filename)\n",
    "        input_flat[k], test_data[k] = raw[\"data\"], (raw[\"test_t\"], raw[\"test_ttau\"])\n",
    "        input_data[k] = unflatten(input_flat[k], lengths=lengths[k])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T20:49:45.270166Z",
     "start_time": "2020-11-23T20:49:45.266150Z"
    }
   },
   "source": [
    "global_space = OrderedDict(\n",
    "    depth=[2, 4, 8],\n",
    "    width=[128, 512, 1024],\n",
    "    learning_rate=[5e-2, 1e-2, 5e-3],\n",
    "    regularization=[1e-6, 1e-8, 1e-10],\n",
    "    dropout=[0.0, 0.1],\n",
    "    lr_factor=[5e-2, 5e-3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T20:49:45.270166Z",
     "start_time": "2020-11-23T20:49:45.266150Z"
    }
   },
   "outputs": [],
   "source": [
    "global_space = OrderedDict(\n",
    "    depth=[3, 4, 5],\n",
    "    width=[256, 512, 768],\n",
    "    learning_rate=[5e-2, 1e-2],\n",
    "    regularization=[1e-5, 1e-6, 1e-7],\n",
    "    dropout=[0.0],\n",
    "    lr_factor=[5e-3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T20:49:00.685583Z",
     "start_time": "2020-11-23T20:49:00.671887Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(space):\n",
    "    generator = DataGenerator(input_data[k], dt=dt, max_frames=500000)\n",
    "    tests = test_data[k], np.zeros((test_data[k][0].shape[0], 2 * n))\n",
    "    its = np.empty((n_runs, n - 1, len(lags)))\n",
    "    scores = np.empty(n_runs)\n",
    "    for i in range(n_runs):\n",
    "        try:\n",
    "            koop = KoopmanModel(n=n, network_lag=lag, verbose=1, nnargs=dict(\n",
    "                width=space[\"width\"], depth=space[\"depth\"], learning_rate=space[\"learning_rate\"],\n",
    "                regularization=space[\"regularization\"], dropout=space[\"dropout\"],\n",
    "                batchnorm=False, lr_factor=space[\"lr_factor\"]))\n",
    "            koop.fit(generator)\n",
    "            its[i] = koop.its(lags)\n",
    "            scores[i] = koop.score(tests)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR\")\n",
    "            traceback.print_exc()\n",
    "            print()\n",
    "        finally:\n",
    "            generator.regenerate_indices()\n",
    "            del koop\n",
    "            gc.collect()\n",
    "    return its, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T20:21:06.429003Z",
     "start_time": "2020-11-23T20:21:06.425995Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_number(val):\n",
    "    return float(val) if \".\" in val or \"e\" in val else int(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T23:06:54.380238Z",
     "start_time": "2020-11-23T20:50:18.900493Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL BEGIN\n",
      "=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~\n",
      "Parameters:\n",
      "  lr_factor: \t\t0.005\n",
      "  width: \t\t128\n",
      "  regularization: \t\t1e-10\n",
      "  depth: \t\t2\n",
      "  learning_rate: \t\t0.01\n",
      "  dropout: \t\t0.0\n",
      "\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/100\n",
      "450000/450000 [==============================] - 4s 8us/sample - loss: -5.9677e-05 - metric_VAMP: 2.3882 - val_loss: -6.0604e-05 - val_metric_VAMP: 2.4253\n",
      "Epoch 2/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.0924e-05 - metric_VAMP: 2.4381 - val_loss: -6.1216e-05 - val_metric_VAMP: 2.4497\n",
      "Epoch 3/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1147e-05 - metric_VAMP: 2.4470 - val_loss: -6.1160e-05 - val_metric_VAMP: 2.4475\n",
      "Epoch 4/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.0997e-05 - metric_VAMP: 2.4410 - val_loss: -6.0982e-05 - val_metric_VAMP: 2.4404\n",
      "Epoch 5/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0800e-05 - metric_VAMP: 2.4331 - val_loss: -6.0827e-05 - val_metric_VAMP: 2.4341\n",
      "Epoch 6/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.0656e-05 - metric_VAMP: 2.4273 - val_loss: -6.0746e-05 - val_metric_VAMP: 2.4309\n",
      "Epoch 7/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0578e-05 - metric_VAMP: 2.4242 - val_loss: -6.0729e-05 - val_metric_VAMP: 2.4302\n",
      "Epoch 8/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.0562e-05 - metric_VAMP: 2.4236 - val_loss: -6.0759e-05 - val_metric_VAMP: 2.4314\n",
      "Epoch 9/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.0588e-05 - metric_VAMP: 2.4246 - val_loss: -6.0822e-05 - val_metric_VAMP: 2.4340\n",
      "Epoch 10/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.0640e-05 - metric_VAMP: 2.4267 - val_loss: -6.0901e-05 - val_metric_VAMP: 2.4371\n",
      "Epoch 11/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.0714e-05 - metric_VAMP: 2.4297 - val_loss: -6.0989e-05 - val_metric_VAMP: 2.4407\n",
      "Epoch 12/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.0795e-05 - metric_VAMP: 2.4329 - val_loss: -6.1077e-05 - val_metric_VAMP: 2.4442\n",
      "Epoch 13/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.0876e-05 - metric_VAMP: 2.4361 - val_loss: -6.1163e-05 - val_metric_VAMP: 2.4476\n",
      "Epoch 14/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.0955e-05 - metric_VAMP: 2.4393 - val_loss: -6.1243e-05 - val_metric_VAMP: 2.4508\n",
      "Epoch 15/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.1031e-05 - metric_VAMP: 2.4424 - val_loss: -6.1317e-05 - val_metric_VAMP: 2.4538\n",
      "Epoch 16/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.1103e-05 - metric_VAMP: 2.4452 - val_loss: -6.1386e-05 - val_metric_VAMP: 2.4565\n",
      "Epoch 17/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.1171e-05 - metric_VAMP: 2.4479 - val_loss: -6.1453e-05 - val_metric_VAMP: 2.4592\n",
      "Epoch 18/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.1240e-05 - metric_VAMP: 2.4507 - val_loss: -6.1517e-05 - val_metric_VAMP: 2.4618\n",
      "Epoch 19/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.1304e-05 - metric_VAMP: 2.4533 - val_loss: -6.1576e-05 - val_metric_VAMP: 2.4641\n",
      "Epoch 20/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1367e-05 - metric_VAMP: 2.4558 - val_loss: -6.1640e-05 - val_metric_VAMP: 2.4667\n",
      "Epoch 21/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1430e-05 - metric_VAMP: 2.4583 - val_loss: -6.1696e-05 - val_metric_VAMP: 2.4689\n",
      "Epoch 22/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1490e-05 - metric_VAMP: 2.4607 - val_loss: -6.1755e-05 - val_metric_VAMP: 2.4713\n",
      "Epoch 23/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1548e-05 - metric_VAMP: 2.4630 - val_loss: -6.1806e-05 - val_metric_VAMP: 2.4733\n",
      "Epoch 24/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1603e-05 - metric_VAMP: 2.4652 - val_loss: -6.1861e-05 - val_metric_VAMP: 2.4755\n",
      "Epoch 25/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1656e-05 - metric_VAMP: 2.4673 - val_loss: -6.1912e-05 - val_metric_VAMP: 2.4776\n",
      "Epoch 26/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1708e-05 - metric_VAMP: 2.4694 - val_loss: -6.1958e-05 - val_metric_VAMP: 2.4794\n",
      "Epoch 27/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1758e-05 - metric_VAMP: 2.4714 - val_loss: -6.2006e-05 - val_metric_VAMP: 2.4813\n",
      "Epoch 28/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1804e-05 - metric_VAMP: 2.4733 - val_loss: -6.2052e-05 - val_metric_VAMP: 2.4832\n",
      "Epoch 29/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1850e-05 - metric_VAMP: 2.4751 - val_loss: -6.2092e-05 - val_metric_VAMP: 2.4848\n",
      "Epoch 30/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1894e-05 - metric_VAMP: 2.4769 - val_loss: -6.2135e-05 - val_metric_VAMP: 2.4865\n",
      "Epoch 31/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1995e-05 - metric_VAMP: 2.4785 - val_loss: -6.2172e-05 - val_metric_VAMP: 2.4880\n",
      "Epoch 32/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1977e-05 - metric_VAMP: 2.4802 - val_loss: -6.2208e-05 - val_metric_VAMP: 2.4894\n",
      "Epoch 33/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2014e-05 - metric_VAMP: 2.4817 - val_loss: -6.2241e-05 - val_metric_VAMP: 2.4907\n",
      "Epoch 34/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2049e-05 - metric_VAMP: 2.4831 - val_loss: -6.2276e-05 - val_metric_VAMP: 2.4921\n",
      "Epoch 35/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2086e-05 - metric_VAMP: 2.4846 - val_loss: -6.2306e-05 - val_metric_VAMP: 2.493305 - metric_\n",
      "Epoch 36/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2119e-05 - metric_VAMP: 2.4859 - val_loss: -6.2336e-05 - val_metric_VAMP: 2.4945\n",
      "Epoch 37/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2153e-05 - metric_VAMP: 2.4872 - val_loss: -6.2366e-05 - val_metric_VAMP: 2.4957\n",
      "Epoch 38/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2185e-05 - metric_VAMP: 2.4885 - val_loss: -6.2395e-05 - val_metric_VAMP: 2.4969\n",
      "Epoch 39/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2218e-05 - metric_VAMP: 2.4898 - val_loss: -6.2422e-05 - val_metric_VAMP: 2.4980\n",
      "Epoch 40/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2248e-05 - metric_VAMP: 2.4910 - val_loss: -6.2448e-05 - val_metric_VAMP: 2.4990\n",
      "Epoch 41/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2280e-05 - metric_VAMP: 2.4923 - val_loss: -6.2474e-05 - val_metric_VAMP: 2.5001\n",
      "Epoch 42/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2310e-05 - metric_VAMP: 2.4935 - val_loss: -6.2500e-05 - val_metric_VAMP: 2.5011\n",
      "Epoch 43/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2342e-05 - metric_VAMP: 2.4948 - val_loss: -6.2528e-05 - val_metric_VAMP: 2.5022\n",
      "Epoch 44/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2373e-05 - metric_VAMP: 2.4960 - val_loss: -6.2555e-05 - val_metric_VAMP: 2.5033\n",
      "Epoch 45/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2404e-05 - metric_VAMP: 2.4973 - val_loss: -6.2579e-05 - val_metric_VAMP: 2.5043\n",
      "Epoch 46/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2435e-05 - metric_VAMP: 2.4985 - val_loss: -6.2607e-05 - val_metric_VAMP: 2.5054\n",
      "Epoch 47/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2465e-05 - metric_VAMP: 2.4997 - val_loss: -6.2634e-05 - val_metric_VAMP: 2.5065\n",
      "Epoch 48/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2498e-05 - metric_VAMP: 2.5010 - val_loss: -6.2660e-05 - val_metric_VAMP: 2.5075\n",
      "Epoch 49/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2529e-05 - metric_VAMP: 2.5023 - val_loss: -6.2690e-05 - val_metric_VAMP: 2.5087\n",
      "Epoch 50/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2560e-05 - metric_VAMP: 2.5035 - val_loss: -6.2717e-05 - val_metric_VAMP: 2.5098\n",
      "Epoch 51/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2499e-05 - metric_VAMP: 2.5048 - val_loss: -6.2743e-05 - val_metric_VAMP: 2.5108\n",
      "Epoch 52/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2626e-05 - metric_VAMP: 2.5061 - val_loss: -6.2775e-05 - val_metric_VAMP: 2.5121\n",
      "Epoch 53/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2659e-05 - metric_VAMP: 2.5075 - val_loss: -6.2802e-05 - val_metric_VAMP: 2.5132\n",
      "Epoch 54/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2692e-05 - metric_VAMP: 2.5088 - val_loss: -6.2832e-05 - val_metric_VAMP: 2.5144\n",
      "Epoch 55/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.2684e-05 - metric_VAMP: 2.5102 - val_loss: -6.2863e-05 - val_metric_VAMP: 2.5156\n",
      "Epoch 56/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.2761e-05 - metric_VAMP: 2.5115 - val_loss: -6.2894e-05 - val_metric_VAMP: 2.5168\n",
      "Epoch 57/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.2795e-05 - metric_VAMP: 2.5129 - val_loss: -6.2925e-05 - val_metric_VAMP: 2.5181\n",
      "Epoch 58/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.2830e-05 - metric_VAMP: 2.5143 - val_loss: -6.2956e-05 - val_metric_VAMP: 2.5193\n",
      "Epoch 59/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.2850e-05 - metric_VAMP: 2.5158 - val_loss: -6.2990e-05 - val_metric_VAMP: 2.5207\n",
      "Epoch 60/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.2968e-05 - metric_VAMP: 2.5171 - val_loss: -6.3023e-05 - val_metric_VAMP: 2.5220\n",
      "Epoch 61/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3049e-05 - metric_VAMP: 2.5186 - val_loss: -6.3054e-05 - val_metric_VAMP: 2.5232\n",
      "Epoch 62/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3011e-05 - metric_VAMP: 2.5199 - val_loss: -6.3087e-05 - val_metric_VAMP: 2.5246\n",
      "Epoch 63/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3007e-05 - metric_VAMP: 2.5214 - val_loss: -6.3117e-05 - val_metric_VAMP: 2.5257\n",
      "Epoch 64/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3041e-05 - metric_VAMP: 2.5227 - val_loss: -6.3148e-05 - val_metric_VAMP: 2.5270\n",
      "Epoch 65/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3075e-05 - metric_VAMP: 2.5241 - val_loss: -6.3179e-05 - val_metric_VAMP: 2.5283\n",
      "Epoch 66/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3172e-05 - metric_VAMP: 2.5255 - val_loss: -6.3210e-05 - val_metric_VAMP: 2.5295\n",
      "Epoch 67/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3142e-05 - metric_VAMP: 2.5268 - val_loss: -6.3241e-05 - val_metric_VAMP: 2.5307\n",
      "Epoch 68/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3175e-05 - metric_VAMP: 2.5281 - val_loss: -6.3273e-05 - val_metric_VAMP: 2.5320\n",
      "Epoch 69/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3251e-05 - metric_VAMP: 2.5307 - val_loss: -6.3300e-05 - val_metric_VAMP: 2.5331\n",
      "Epoch 70/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3238e-05 - metric_VAMP: 2.5306 - val_loss: -6.3327e-05 - val_metric_VAMP: 2.5342\n",
      "Epoch 71/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3269e-05 - metric_VAMP: 2.5319 - val_loss: -6.3357e-05 - val_metric_VAMP: 2.5354\n",
      "Epoch 72/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3374e-05 - metric_VAMP: 2.5331 - val_loss: -6.3380e-05 - val_metric_VAMP: 2.5363\n",
      "Epoch 73/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3328e-05 - metric_VAMP: 2.5342 - val_loss: -6.3404e-05 - val_metric_VAMP: 2.5373\n",
      "Epoch 74/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3358e-05 - metric_VAMP: 2.5354 - val_loss: -6.3435e-05 - val_metric_VAMP: 2.5385\n",
      "Epoch 75/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3405e-05 - metric_VAMP: 2.5366 - val_loss: -6.3459e-05 - val_metric_VAMP: 2.5394\n",
      "Epoch 76/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3415e-05 - metric_VAMP: 2.5377 - val_loss: -6.3482e-05 - val_metric_VAMP: 2.5403\n",
      "Epoch 77/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3300e-05 - metric_VAMP: 2.5388 - val_loss: -6.3508e-05 - val_metric_VAMP: 2.5414\n",
      "Epoch 78/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3429e-05 - metric_VAMP: 2.5399 - val_loss: -6.3531e-05 - val_metric_VAMP: 2.5423\n",
      "Epoch 79/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3497e-05 - metric_VAMP: 2.5410 - val_loss: -6.3554e-05 - val_metric_VAMP: 2.5432\n",
      "Epoch 80/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3522e-05 - metric_VAMP: 2.5420 - val_loss: -6.3578e-05 - val_metric_VAMP: 2.5442\n",
      "Epoch 81/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3553e-05 - metric_VAMP: 2.5430 - val_loss: -6.3601e-05 - val_metric_VAMP: 2.5451\n",
      "Epoch 82/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3581e-05 - metric_VAMP: 2.5440 - val_loss: -6.3623e-05 - val_metric_VAMP: 2.5460\n",
      "Epoch 83/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3598e-05 - metric_VAMP: 2.5450 - val_loss: -6.3645e-05 - val_metric_VAMP: 2.5469\n",
      "Epoch 84/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3623e-05 - metric_VAMP: 2.5460 - val_loss: -6.3665e-05 - val_metric_VAMP: 2.5477\n",
      "Epoch 85/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3687e-05 - metric_VAMP: 2.5470 - val_loss: -6.3687e-05 - val_metric_VAMP: 2.5486\n",
      "Epoch 86/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3670e-05 - metric_VAMP: 2.5479 - val_loss: -6.3705e-05 - val_metric_VAMP: 2.6013\n",
      "Epoch 87/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3683e-05 - metric_VAMP: 2.5488 - val_loss: -6.3728e-05 - val_metric_VAMP: 2.5502\n",
      "Epoch 88/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3717e-05 - metric_VAMP: 2.5498 - val_loss: -6.3747e-05 - val_metric_VAMP: 2.5510\n",
      "Epoch 89/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3754e-05 - metric_VAMP: 2.5507 - val_loss: -6.3766e-05 - val_metric_VAMP: 2.5517\n",
      "Epoch 90/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3761e-05 - metric_VAMP: 2.5515 - val_loss: -6.3784e-05 - val_metric_VAMP: 2.5524\n",
      "Epoch 91/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3783e-05 - metric_VAMP: 2.5524 - val_loss: -6.3803e-05 - val_metric_VAMP: 2.5532\n",
      "Epoch 92/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3804e-05 - metric_VAMP: 2.5533 - val_loss: -6.3820e-05 - val_metric_VAMP: 2.5539\n",
      "Epoch 93/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3826e-05 - metric_VAMP: 2.5541 - val_loss: -6.3833e-05 - val_metric_VAMP: 2.5544\n",
      "Epoch 94/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3793e-05 - metric_VAMP: 2.5549 - val_loss: -6.3853e-05 - val_metric_VAMP: 2.5552\n",
      "Epoch 95/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3866e-05 - metric_VAMP: 2.5561 - val_loss: -6.3871e-05 - val_metric_VAMP: 2.5559\n",
      "Epoch 96/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3843e-05 - metric_VAMP: 2.5623 - val_loss: -6.3885e-05 - val_metric_VAMP: 2.5565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3967e-05 - metric_VAMP: 2.5680 - val_loss: -6.3903e-05 - val_metric_VAMP: 2.5572\n",
      "Epoch 98/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.4028e-05 - metric_VAMP: 2.5719 - val_loss: -6.3916e-05 - val_metric_VAMP: 2.5577\n",
      "Epoch 99/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -6.3625e-05 - metric_VAMP: 2.5589 - val_loss: -6.3931e-05 - val_metric_VAMP: 2.5584\n",
      "Epoch 100/100\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -6.3944e-05 - metric_VAMP: 2.5597 - val_loss: -6.3949e-05 - val_metric_VAMP: 2.5591\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/100\n",
      "450000/450000 [==============================] - 4s 8us/sample - loss: -2.2737 - metric_VAMP: 2.6112 - val_loss: -2.3665 - val_metric_VAMP: 2.6643\n",
      "Epoch 2/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.4453 - metric_VAMP: 2.7083 - val_loss: -2.5022 - val_metric_VAMP: 2.7396\n",
      "Epoch 3/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.5521 - metric_VAMP: 2.7668 - val_loss: -2.5842 - val_metric_VAMP: 2.7842\n",
      "Epoch 4/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.6085 - metric_VAMP: 2.7973 - val_loss: -2.6286 - val_metric_VAMP: 2.8080\n",
      "Epoch 5/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.6457 - metric_VAMP: 2.8171 - val_loss: -2.6542 - val_metric_VAMP: 2.8217\n",
      "Epoch 6/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.6701 - metric_VAMP: 2.8301 - val_loss: -2.6756 - val_metric_VAMP: 2.8330\n",
      "Epoch 7/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.6896 - metric_VAMP: 2.8405 - val_loss: -2.6921 - val_metric_VAMP: 2.8418\n",
      "Epoch 8/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7051 - metric_VAMP: 2.8487 - val_loss: -2.7052 - val_metric_VAMP: 2.8487\n",
      "Epoch 9/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7184 - metric_VAMP: 2.8557 - val_loss: -2.7168 - val_metric_VAMP: 2.8548\n",
      "Epoch 10/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7295 - metric_VAMP: 2.8615 - val_loss: -2.7269 - val_metric_VAMP: 2.8601\n",
      "Epoch 11/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7392 - metric_VAMP: 2.8666 - val_loss: -2.7357 - val_metric_VAMP: 2.8647\n",
      "Epoch 12/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7480 - metric_VAMP: 2.8712 - val_loss: -2.7435 - val_metric_VAMP: 2.8688\n",
      "Epoch 13/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7560 - metric_VAMP: 2.8754 - val_loss: -2.7510 - val_metric_VAMP: 2.8727\n",
      "Epoch 14/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7632 - metric_VAMP: 2.8791 - val_loss: -2.7580 - val_metric_VAMP: 2.8764\n",
      "Epoch 15/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7699 - metric_VAMP: 2.8826 - val_loss: -2.7634 - val_metric_VAMP: 2.8792\n",
      "Epoch 16/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7759 - metric_VAMP: 2.8857 - val_loss: -2.7682 - val_metric_VAMP: 2.8817\n",
      "Epoch 17/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7816 - metric_VAMP: 2.8887 - val_loss: -2.7740 - val_metric_VAMP: 2.8848\n",
      "Epoch 18/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7870 - metric_VAMP: 2.8915 - val_loss: -2.7790 - val_metric_VAMP: 2.8874\n",
      "Epoch 19/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7920 - metric_VAMP: 2.8941 - val_loss: -2.7841 - val_metric_VAMP: 2.8900\n",
      "Epoch 20/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7967 - metric_VAMP: 2.8966 - val_loss: -2.7882 - val_metric_VAMP: 2.8921\n",
      "Epoch 21/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8011 - metric_VAMP: 2.8988 - val_loss: -2.7917 - val_metric_VAMP: 2.8940\n",
      "Epoch 22/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8052 - metric_VAMP: 2.9010 - val_loss: -2.7958 - val_metric_VAMP: 2.8961\n",
      "Epoch 23/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8090 - metric_VAMP: 2.9029 - val_loss: -2.7991 - val_metric_VAMP: 2.8978\n",
      "Epoch 24/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8129 - metric_VAMP: 2.9049 - val_loss: -2.8021 - val_metric_VAMP: 2.8993\n",
      "Epoch 25/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8164 - metric_VAMP: 2.9067 - val_loss: -2.8054 - val_metric_VAMP: 2.9010\n",
      "Epoch 26/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8198 - metric_VAMP: 2.9085 - val_loss: -2.8082 - val_metric_VAMP: 2.9025\n",
      "Epoch 27/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8231 - metric_VAMP: 2.9102 - val_loss: -2.8117 - val_metric_VAMP: 2.9043\n",
      "Epoch 28/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8264 - metric_VAMP: 2.9119 - val_loss: -2.8149 - val_metric_VAMP: 2.9060\n",
      "Epoch 29/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8293 - metric_VAMP: 2.9134 - val_loss: -2.8171 - val_metric_VAMP: 2.9071\n",
      "Epoch 30/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8322 - metric_VAMP: 2.9149 - val_loss: -2.8206 - val_metric_VAMP: 2.9089\n",
      "Epoch 31/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8349 - metric_VAMP: 2.9163 - val_loss: -2.8227 - val_metric_VAMP: 2.9100\n",
      "Epoch 32/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8376 - metric_VAMP: 2.9176 - val_loss: -2.8249 - val_metric_VAMP: 2.9111\n",
      "Epoch 33/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -2.8403 - metric_VAMP: 2.9190 - val_loss: -2.8275 - val_metric_VAMP: 2.9124\n",
      "Epoch 34/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8428 - metric_VAMP: 2.9203 - val_loss: -2.8298 - val_metric_VAMP: 2.9136\n",
      "Epoch 35/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8451 - metric_VAMP: 2.9215 - val_loss: -2.8314 - val_metric_VAMP: 2.9145\n",
      "Epoch 36/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8474 - metric_VAMP: 2.9227 - val_loss: -2.8341 - val_metric_VAMP: 2.9158\n",
      "Epoch 37/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8497 - metric_VAMP: 2.9239 - val_loss: -2.8361 - val_metric_VAMP: 2.9169\n",
      "Epoch 38/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8519 - metric_VAMP: 2.9250 - val_loss: -2.8380 - val_metric_VAMP: 2.9178\n",
      "Epoch 39/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8540 - metric_VAMP: 2.9261 - val_loss: -2.8404 - val_metric_VAMP: 2.9191\n",
      "Epoch 40/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8561 - metric_VAMP: 2.9272 - val_loss: -2.8418 - val_metric_VAMP: 2.9198\n",
      "Epoch 41/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8580 - metric_VAMP: 2.9281 - val_loss: -2.8440 - val_metric_VAMP: 2.9209\n",
      "Epoch 42/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8600 - metric_VAMP: 2.9292 - val_loss: -2.8458 - val_metric_VAMP: 2.9219\n",
      "Epoch 43/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8621 - metric_VAMP: 2.9302 - val_loss: -2.8468 - val_metric_VAMP: 2.9224\n",
      "Epoch 44/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8638 - metric_VAMP: 2.9311 - val_loss: -2.8488 - val_metric_VAMP: 2.9234\n",
      "Epoch 45/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8661 - metric_VAMP: 2.9323 - val_loss: -2.8514 - val_metric_VAMP: 2.9247\n",
      "Epoch 46/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8679 - metric_VAMP: 2.9332 - val_loss: -2.8527 - val_metric_VAMP: 2.9254\n",
      "Epoch 47/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8699 - metric_VAMP: 2.9342 - val_loss: -2.8541 - val_metric_VAMP: 2.9261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8711 - metric_VAMP: 2.9348 - val_loss: -2.8550 - val_metric_VAMP: 2.9266\n",
      "Epoch 49/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8727 - metric_VAMP: 2.9356 - val_loss: -2.8561 - val_metric_VAMP: 2.9272\n",
      "Epoch 50/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8744 - metric_VAMP: 2.9365 - val_loss: -2.8587 - val_metric_VAMP: 2.9285\n",
      "Epoch 51/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8762 - metric_VAMP: 2.9375 - val_loss: -2.8600 - val_metric_VAMP: 2.9291\n",
      "Epoch 52/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8779 - metric_VAMP: 2.9383 - val_loss: -2.8610 - val_metric_VAMP: 2.9297\n",
      "Epoch 53/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8793 - metric_VAMP: 2.9390 - val_loss: -2.8628 - val_metric_VAMP: 2.9306\n",
      "Epoch 54/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8810 - metric_VAMP: 2.9399 - val_loss: -2.8643 - val_metric_VAMP: 2.9313\n",
      "Epoch 55/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8825 - metric_VAMP: 2.9406 - val_loss: -2.8662 - val_metric_VAMP: 2.9323\n",
      "Epoch 56/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8838 - metric_VAMP: 2.9413 - val_loss: -2.8672 - val_metric_VAMP: 2.9328\n",
      "Epoch 57/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8851 - metric_VAMP: 2.9420 - val_loss: -2.8684 - val_metric_VAMP: 2.9335\n",
      "Epoch 58/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8865 - metric_VAMP: 2.9427 - val_loss: -2.8697 - val_metric_VAMP: 2.9341\n",
      "Epoch 59/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8880 - metric_VAMP: 2.9435 - val_loss: -2.8706 - val_metric_VAMP: 2.9346\n",
      "Epoch 60/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8893 - metric_VAMP: 2.9441 - val_loss: -2.8720 - val_metric_VAMP: 2.9353\n",
      "Epoch 61/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -2.8906 - metric_VAMP: 2.9448 - val_loss: -2.8731 - val_metric_VAMP: 2.9359\n",
      "Epoch 62/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8917 - metric_VAMP: 2.9453 - val_loss: -2.8742 - val_metric_VAMP: 2.9364\n",
      "Epoch 63/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8931 - metric_VAMP: 2.9460 - val_loss: -2.8757 - val_metric_VAMP: 2.9372\n",
      "Epoch 64/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8944 - metric_VAMP: 2.9467 - val_loss: -2.8758 - val_metric_VAMP: 2.9372\n",
      "Epoch 65/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8957 - metric_VAMP: 2.9474 - val_loss: -2.8775 - val_metric_VAMP: 2.9381\n",
      "Epoch 66/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8969 - metric_VAMP: 2.9480 - val_loss: -2.8790 - val_metric_VAMP: 2.9389\n",
      "Epoch 67/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8980 - metric_VAMP: 2.9485 - val_loss: -2.8794 - val_metric_VAMP: 2.9390\n",
      "Epoch 68/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8992 - metric_VAMP: 2.9491 - val_loss: -2.8813 - val_metric_VAMP: 2.9400\n",
      "Epoch 69/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9003 - metric_VAMP: 2.9497 - val_loss: -2.8818 - val_metric_VAMP: 2.9403\n",
      "Epoch 70/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9015 - metric_VAMP: 2.9503 - val_loss: -2.8828 - val_metric_VAMP: 2.9408\n",
      "Epoch 71/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9026 - metric_VAMP: 2.9509 - val_loss: -2.8840 - val_metric_VAMP: 2.9414\n",
      "Epoch 72/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9035 - metric_VAMP: 2.9514 - val_loss: -2.8848 - val_metric_VAMP: 2.9418\n",
      "Epoch 73/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -2.9044 - metric_VAMP: 2.9518 - val_loss: -2.8851 - val_metric_VAMP: 2.9420\n",
      "Epoch 74/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9053 - metric_VAMP: 2.9523 - val_loss: -2.8859 - val_metric_VAMP: 2.9424\n",
      "Epoch 75/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9064 - metric_VAMP: 2.9528 - val_loss: -2.8878 - val_metric_VAMP: 2.9433\n",
      "Epoch 76/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9076 - metric_VAMP: 2.9534 - val_loss: -2.8872 - val_metric_VAMP: 2.9431\n",
      "Epoch 77/100\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -2.9084 - metric_VAMP: 2.9538 - val_loss: -2.8890 - val_metric_VAMP: 2.9440\n",
      "Epoch 78/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9097 - metric_VAMP: 2.9545 - val_loss: -2.8898 - val_metric_VAMP: 2.9443\n",
      "Epoch 79/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9106 - metric_VAMP: 2.9550 - val_loss: -2.8909 - val_metric_VAMP: 2.9449\n",
      "Epoch 80/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9116 - metric_VAMP: 2.9555 - val_loss: -2.8914 - val_metric_VAMP: 2.9452\n",
      "Epoch 81/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9126 - metric_VAMP: 2.9560 - val_loss: -2.8927 - val_metric_VAMP: 2.9458\n",
      "Epoch 82/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9137 - metric_VAMP: 2.9565 - val_loss: -2.8933 - val_metric_VAMP: 2.9462\n",
      "Epoch 83/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9147 - metric_VAMP: 2.9570 - val_loss: -2.8942 - val_metric_VAMP: 2.9466\n",
      "Epoch 84/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9156 - metric_VAMP: 2.9575 - val_loss: -2.8945 - val_metric_VAMP: 2.9467\n",
      "Epoch 85/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9165 - metric_VAMP: 2.9579 - val_loss: -2.8953 - val_metric_VAMP: 2.9472\n",
      "Epoch 86/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9172 - metric_VAMP: 2.9583 - val_loss: -2.8962 - val_metric_VAMP: 2.9476\n",
      "Epoch 87/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9179 - metric_VAMP: 2.9587 - val_loss: -2.8967 - val_metric_VAMP: 2.9479\n",
      "Epoch 88/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9189 - metric_VAMP: 2.9592 - val_loss: -2.8978 - val_metric_VAMP: 2.9484\n",
      "Epoch 89/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9197 - metric_VAMP: 2.9595 - val_loss: -2.8978 - val_metric_VAMP: 2.9485\n",
      "Epoch 90/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9204 - metric_VAMP: 2.9599 - val_loss: -2.8984 - val_metric_VAMP: 2.9487\n",
      "Epoch 91/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9209 - metric_VAMP: 2.9602 - val_loss: -2.8997 - val_metric_VAMP: 2.9494\n",
      "Epoch 92/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9218 - metric_VAMP: 2.9607 - val_loss: -2.8997 - val_metric_VAMP: 2.9494\n",
      "Epoch 93/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9226 - metric_VAMP: 2.9611 - val_loss: -2.9008 - val_metric_VAMP: 2.9500\n",
      "Epoch 94/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9234 - metric_VAMP: 2.9614 - val_loss: -2.9009 - val_metric_VAMP: 2.9500\n",
      "Epoch 95/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9243 - metric_VAMP: 2.9619 - val_loss: -2.9020 - val_metric_VAMP: 2.9506\n",
      "Epoch 96/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9253 - metric_VAMP: 2.9624 - val_loss: -2.9026 - val_metric_VAMP: 2.9509\n",
      "Epoch 97/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9262 - metric_VAMP: 2.9629 - val_loss: -2.9034 - val_metric_VAMP: 2.9513\n",
      "Epoch 98/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9272 - metric_VAMP: 2.9633 - val_loss: -2.9043 - val_metric_VAMP: 2.9518\n",
      "Epoch 99/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9278 - metric_VAMP: 2.9637 - val_loss: -2.9048 - val_metric_VAMP: 2.9520\n",
      "Epoch 100/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9284 - metric_VAMP: 2.9640 - val_loss: -2.9047 - val_metric_VAMP: 2.9520\n",
      "Setting initial auxiliary weights...\n",
      "Training auxiliary network...\n",
      "Both valid loss: -3.7270171642303467\n",
      "Training full network...\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 5s 12us/sample - loss: -3.7975 - val_loss: -3.8003\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8295 - val_loss: -3.8171\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8428 - val_loss: -3.8282\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8526 - val_loss: -3.8357\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8604 - val_loss: -3.8419\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8662 - val_loss: -3.8464\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8709 - val_loss: -3.8503\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8753 - val_loss: -3.8543\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8789 - val_loss: -3.8570\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8821 - val_loss: -3.8600\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8849 - val_loss: -3.8622\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8875 - val_loss: -3.8644\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8897 - val_loss: -3.8660\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8918 - val_loss: -3.8678\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8937 - val_loss: -3.8694\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8954 - val_loss: -3.8706\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8971 - val_loss: -3.8721\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.8987 - val_loss: -3.8736\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9002 - val_loss: -3.8741\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9015 - val_loss: -3.8755\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9028 - val_loss: -3.8764\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9040 - val_loss: -3.8772\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9051 - val_loss: -3.8782\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9061 - val_loss: -3.8788\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9071 - val_loss: -3.8793\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9081 - val_loss: -3.8804\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9090 - val_loss: -3.8806\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9099 - val_loss: -3.8814\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9107 - val_loss: -3.8819\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9115 - val_loss: -3.8823\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9122 - val_loss: -3.8831\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9131 - val_loss: -3.8833\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9137 - val_loss: -3.8839\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9144 - val_loss: -3.8845\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9152 - val_loss: -3.8846\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9157 - val_loss: -3.8854\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9163 - val_loss: -3.8855\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9168 - val_loss: -3.8856\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9174 - val_loss: -3.8862\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9179 - val_loss: -3.8861\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9186 - val_loss: -3.8870\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9190 - val_loss: -3.8869\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9195 - val_loss: -3.8868\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9198 - val_loss: -3.8876\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9204 - val_loss: -3.8881\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9210 - val_loss: -3.8875\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9214 - val_loss: -3.8883\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9219 - val_loss: -3.8886\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9221 - val_loss: -3.8886\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9227 - val_loss: -3.8889\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9232 - val_loss: -3.8892\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9236 - val_loss: -3.8893\n",
      "Epoch 53/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9239 - val_loss: -3.8895\n",
      "Epoch 54/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9243 - val_loss: -3.8895\n",
      "Epoch 55/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9247 - val_loss: -3.8901\n",
      "Epoch 56/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9249 - val_loss: -3.8899\n",
      "Epoch 57/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9252 - val_loss: -3.8895\n",
      "Epoch 58/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9256 - val_loss: -3.8902\n",
      "Epoch 59/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9260 - val_loss: -3.8905\n",
      "Epoch 60/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9263 - val_loss: -3.8907\n",
      "Epoch 61/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9266 - val_loss: -3.8901\n",
      "Epoch 62/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9268 - val_loss: -3.8905\n",
      "Epoch 63/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9272 - val_loss: -3.8904\n",
      "Epoch 64/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9274 - val_loss: -3.8907\n",
      "Epoch 65/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9278 - val_loss: -3.8913\n",
      "Epoch 66/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9281 - val_loss: -3.8910\n",
      "Epoch 67/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9284 - val_loss: -3.8915\n",
      "Epoch 68/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9288 - val_loss: -3.8914\n",
      "Epoch 69/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9291 - val_loss: -3.8915\n",
      "Epoch 70/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9292 - val_loss: -3.8907\n",
      "Epoch 71/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9295 - val_loss: -3.8916\n",
      "Epoch 72/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9298 - val_loss: -3.8921\n",
      "Epoch 73/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9300 - val_loss: -3.8916\n",
      "Epoch 74/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9303 - val_loss: -3.8915\n",
      "Epoch 75/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9307 - val_loss: -3.8918\n",
      "Epoch 76/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9309 - val_loss: -3.8917\n",
      "Epoch 77/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9310 - val_loss: -3.8919\n",
      "Epoch 78/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9313 - val_loss: -3.8924\n",
      "Epoch 79/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9314 - val_loss: -3.8921\n",
      "Epoch 80/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9315 - val_loss: -3.8921\n",
      "Epoch 81/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9317 - val_loss: -3.8913\n",
      "Epoch 82/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9320 - val_loss: -3.8919\n",
      "Epoch 83/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9324 - val_loss: -3.8926\n",
      "Epoch 84/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9326 - val_loss: -3.8921\n",
      "Epoch 85/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9328 - val_loss: -3.8925\n",
      "Epoch 86/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9331 - val_loss: -3.8925\n",
      "Epoch 87/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9333 - val_loss: -3.8923\n",
      "Epoch 88/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9334 - val_loss: -3.8923\n",
      "Epoch 89/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9336 - val_loss: -3.8922\n",
      "Epoch 90/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9339 - val_loss: -3.8928\n",
      "Epoch 91/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9341 - val_loss: -3.8929\n",
      "Epoch 92/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9343 - val_loss: -3.8926\n",
      "Epoch 93/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9346 - val_loss: -3.8927\n",
      "Epoch 94/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9347 - val_loss: -3.8928\n",
      "Epoch 95/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9348 - val_loss: -3.8923\n",
      "Epoch 96/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9350 - val_loss: -3.8926\n",
      "Epoch 97/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9350 - val_loss: -3.8928\n",
      "Epoch 98/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9354 - val_loss: -3.8928\n",
      "Epoch 99/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9356 - val_loss: -3.8925\n",
      "Epoch 100/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9358 - val_loss: -3.8930\n",
      "Epoch 101/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9359 - val_loss: -3.8931\n",
      "Epoch 102/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9361 - val_loss: -3.8929\n",
      "Epoch 103/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9361 - val_loss: -3.8932\n",
      "Epoch 104/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9364 - val_loss: -3.8927\n",
      "Epoch 105/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9366 - val_loss: -3.8931\n",
      "Epoch 106/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9366 - val_loss: -3.8930\n",
      "Epoch 107/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9369 - val_loss: -3.8931\n",
      "Epoch 108/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9369 - val_loss: -3.8935\n",
      "Epoch 109/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9372 - val_loss: -3.8929\n",
      "Epoch 110/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9372 - val_loss: -3.8926\n",
      "Epoch 111/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9374 - val_loss: -3.8929\n",
      "Epoch 112/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9375 - val_loss: -3.8930\n",
      "Epoch 113/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9378 - val_loss: -3.8931\n",
      "Epoch 114/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9379 - val_loss: -3.8933\n",
      "Epoch 115/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9380 - val_loss: -3.8933\n",
      "Epoch 116/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9381 - val_loss: -3.8929\n",
      "Epoch 117/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9383 - val_loss: -3.8929\n",
      "Epoch 118/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9383 - val_loss: -3.8931\n",
      "Epoch 119/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9386 - val_loss: -3.8931\n",
      "Epoch 120/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9388 - val_loss: -3.8933\n",
      "Epoch 121/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9389 - val_loss: -3.8928\n",
      "Epoch 122/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9389 - val_loss: -3.8933\n",
      "Epoch 123/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9392 - val_loss: -3.8933\n",
      "Epoch 124/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9393 - val_loss: -3.8933\n",
      "Epoch 125/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9395 - val_loss: -3.8933\n",
      "Epoch 126/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9395 - val_loss: -3.8935\n",
      "Epoch 127/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9397 - val_loss: -3.8933\n",
      "Epoch 128/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9399 - val_loss: -3.8934\n",
      "Epoch 129/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9399 - val_loss: -3.8933\n",
      "Epoch 130/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9402 - val_loss: -3.8934\n",
      "Epoch 131/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9403 - val_loss: -3.8935\n",
      "Epoch 132/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9404 - val_loss: -3.8933\n",
      "Epoch 133/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9406 - val_loss: -3.8933\n",
      "Epoch 134/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9408 - val_loss: -3.8932\n",
      "Epoch 135/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9409 - val_loss: -3.8933\n",
      "Epoch 136/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9410 - val_loss: -3.8935\n",
      "Epoch 137/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9411 - val_loss: -3.8938\n",
      "Epoch 138/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9411 - val_loss: -3.8930\n",
      "Epoch 139/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9412 - val_loss: -3.8934\n",
      "Epoch 140/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9413 - val_loss: -3.8934\n",
      "Epoch 141/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9414 - val_loss: -3.8937\n",
      "Epoch 142/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9417 - val_loss: -3.8938\n",
      "Epoch 143/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9417 - val_loss: -3.8938\n",
      "Epoch 144/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9418 - val_loss: -3.8937\n",
      "Epoch 145/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9419 - val_loss: -3.8933\n",
      "Epoch 146/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9421 - val_loss: -3.8936\n",
      "Epoch 147/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9420 - val_loss: -3.8932\n",
      "Epoch 148/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9422 - val_loss: -3.8940\n",
      "Epoch 149/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9424 - val_loss: -3.8936\n",
      "Epoch 150/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9426 - val_loss: -3.8935\n",
      "Epoch 151/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9427 - val_loss: -3.8934\n",
      "Epoch 152/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9427 - val_loss: -3.8938\n",
      "Epoch 153/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9430 - val_loss: -3.8934\n",
      "Epoch 154/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9431 - val_loss: -3.8936\n",
      "Epoch 155/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9431 - val_loss: -3.8933\n",
      "Epoch 156/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9432 - val_loss: -3.8937\n",
      "Epoch 157/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9433 - val_loss: -3.8938\n",
      "Epoch 158/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9433 - val_loss: -3.8930\n",
      "Epoch 159/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9435 - val_loss: -3.8935\n",
      "Epoch 160/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9437 - val_loss: -3.8935\n",
      "Epoch 161/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9438 - val_loss: -3.8936\n",
      "Epoch 162/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9439 - val_loss: -3.8938\n",
      "Epoch 163/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9439 - val_loss: -3.8938\n",
      "Epoch 164/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9439 - val_loss: -3.8936\n",
      "Epoch 165/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9440 - val_loss: -3.8931\n",
      "Epoch 166/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9440 - val_loss: -3.8927\n",
      "Epoch 167/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9441 - val_loss: -3.8935\n",
      "Epoch 168/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9444 - val_loss: -3.8939\n",
      "Epoch 169/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9444 - val_loss: -3.8932\n",
      "Epoch 170/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9445 - val_loss: -3.8932\n",
      "Epoch 171/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9445 - val_loss: -3.8932\n",
      "Epoch 172/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9447 - val_loss: -3.8934\n",
      "Epoch 173/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9448 - val_loss: -3.8933\n",
      "Epoch 174/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9449 - val_loss: -3.8932\n",
      "Epoch 175/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9450 - val_loss: -3.8936\n",
      "Epoch 176/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9451 - val_loss: -3.8934\n",
      "Epoch 177/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9451 - val_loss: -3.8930\n",
      "Epoch 178/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9451 - val_loss: -3.8931\n",
      "Epoch 179/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9452 - val_loss: -3.8931\n",
      "Epoch 180/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9453 - val_loss: -3.8934\n",
      "Epoch 181/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9454 - val_loss: -3.8926\n",
      "Epoch 182/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9454 - val_loss: -3.8934\n",
      "Epoch 183/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9456 - val_loss: -3.8930\n",
      "Epoch 184/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8934\n",
      "Epoch 185/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8924\n",
      "Epoch 186/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8926\n",
      "Epoch 187/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8931\n",
      "Epoch 188/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9459 - val_loss: -3.8931\n",
      "Epoch 189/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8926\n",
      "Epoch 190/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8934\n",
      "Epoch 191/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9463 - val_loss: -3.8929\n",
      "Epoch 192/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9465 - val_loss: -3.8927\n",
      "Epoch 193/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9465 - val_loss: -3.8925\n",
      "Epoch 194/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9466 - val_loss: -3.8931\n",
      "Epoch 195/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9466 - val_loss: -3.8931\n",
      "Epoch 196/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9467 - val_loss: -3.8929\n",
      "Epoch 197/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9469 - val_loss: -3.8927\n",
      "Epoch 198/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9469 - val_loss: -3.8929\n",
      "Setting up auxiliary training loop...\n",
      "Both valid loss: -3.7270171642303467\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 5s 12us/sample - loss: -3.9418 - val_loss: -3.8939\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9423 - val_loss: -3.8940\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9425 - val_loss: -3.8936\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9427 - val_loss: -3.8939\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9428 - val_loss: -3.8934\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9430 - val_loss: -3.8941\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9432 - val_loss: -3.8940\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9433 - val_loss: -3.8938\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9434 - val_loss: -3.8941\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9436 - val_loss: -3.8939\n",
      "Epoch 11/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9436 - val_loss: -3.8939\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9436 - val_loss: -3.8939\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9438 - val_loss: -3.8938\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9439 - val_loss: -3.8936\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9441 - val_loss: -3.8938\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9441 - val_loss: -3.8939\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9442 - val_loss: -3.8936\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9442 - val_loss: -3.8939\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9443 - val_loss: -3.8940\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9445 - val_loss: -3.8935\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9446 - val_loss: -3.8939\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9447 - val_loss: -3.8938\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9447 - val_loss: -3.8935\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9448 - val_loss: -3.8935\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9448 - val_loss: -3.8934\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9450 - val_loss: -3.8939\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9451 - val_loss: -3.8934\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9452 - val_loss: -3.8938\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9453 - val_loss: -3.8935\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9454 - val_loss: -3.8933\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9455 - val_loss: -3.8936\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9455 - val_loss: -3.8935\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8936\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8932\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8939\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8932\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8932\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9459 - val_loss: -3.8932\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9459 - val_loss: -3.8935\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8934\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9462 - val_loss: -3.8934\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9463 - val_loss: -3.8932\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9465 - val_loss: -3.8935\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9466 - val_loss: -3.8932\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9466 - val_loss: -3.8932\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9467 - val_loss: -3.8933\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9468 - val_loss: -3.8928\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9468 - val_loss: -3.8933\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9469 - val_loss: -3.8933\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9470 - val_loss: -3.8934\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9469 - val_loss: -3.8928\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9469 - val_loss: -3.8925\n",
      "Epoch 53/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9469 - val_loss: -3.8934\n",
      "Epoch 54/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9471 - val_loss: -3.8929\n",
      "Epoch 55/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9472 - val_loss: -3.8929\n",
      "Epoch 56/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8929\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: -3.8941\n",
      "Iteration 0: Score: 3.8940722942352295\n",
      "Both valid loss: -3.7270171642303467\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9432 - val_loss: -3.8941\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9434 - val_loss: -3.8939\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9434 - val_loss: -3.8939\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9435 - val_loss: -3.8937\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9436 - val_loss: -3.8937\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9436 - val_loss: -3.8941\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9437 - val_loss: -3.8936\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9439 - val_loss: -3.8934\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9440 - val_loss: -3.8940\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9440 - val_loss: -3.8935\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9439 - val_loss: -3.8935\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9442 - val_loss: -3.8942\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9442 - val_loss: -3.8937\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9444 - val_loss: -3.8934\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9445 - val_loss: -3.8934\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9446 - val_loss: -3.8938\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9447 - val_loss: -3.8934\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9448 - val_loss: -3.8938\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9448 - val_loss: -3.8936\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9449 - val_loss: -3.8932\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9451 - val_loss: -3.8936\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9450 - val_loss: -3.8932\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9451 - val_loss: -3.8935\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9453 - val_loss: -3.8938\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9453 - val_loss: -3.8933\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9455 - val_loss: -3.8935\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8933\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8933\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8932\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8930\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8934\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9459 - val_loss: -3.8933\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8937\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8933\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9460 - val_loss: -3.8930\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8932\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9463 - val_loss: -3.8934\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9464 - val_loss: -3.8932\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9466 - val_loss: -3.8933\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9467 - val_loss: -3.8935\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9468 - val_loss: -3.8930\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9467 - val_loss: -3.8935\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9469 - val_loss: -3.8931\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9469 - val_loss: -3.8930\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9470 - val_loss: -3.8934\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9470 - val_loss: -3.8929\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9470 - val_loss: -3.8928\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9472 - val_loss: -3.8929\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9473 - val_loss: -3.8930\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8933\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9475 - val_loss: -3.8925\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9475 - val_loss: -3.8932\n",
      "Epoch 53/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9476 - val_loss: -3.8928\n",
      "Epoch 54/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9477 - val_loss: -3.8929\n",
      "Epoch 55/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9477 - val_loss: -3.8929\n",
      "Epoch 56/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9478 - val_loss: -3.8930\n",
      "Epoch 57/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9480 - val_loss: -3.8928\n",
      "Epoch 58/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9480 - val_loss: -3.8927\n",
      "Epoch 59/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9480 - val_loss: -3.8927\n",
      "Epoch 60/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9479 - val_loss: -3.8921\n",
      "Epoch 61/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9481 - val_loss: -3.8926\n",
      "Epoch 62/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9481 - val_loss: -3.8927\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: -3.8942\n",
      "Iteration 1: Score: 3.894171857833862\n",
      "Both valid loss: -3.7270171642303467\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9442 - val_loss: -3.8937\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9443 - val_loss: -3.8936\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9444 - val_loss: -3.8939\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9445 - val_loss: -3.8935\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9445 - val_loss: -3.8935\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9447 - val_loss: -3.8938\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9448 - val_loss: -3.8934\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9449 - val_loss: -3.8943\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9449 - val_loss: -3.8936\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9449 - val_loss: -3.8931\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9451 - val_loss: -3.8932\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9452 - val_loss: -3.8937\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9453 - val_loss: -3.8932\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9455 - val_loss: -3.8934\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9455 - val_loss: -3.8935\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9456 - val_loss: -3.8936\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8936\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9457 - val_loss: -3.8930\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8937\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8937\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9459 - val_loss: -3.8924\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8932\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8935\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8933\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9463 - val_loss: -3.8934\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9464 - val_loss: -3.8927\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9466 - val_loss: -3.8934\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9466 - val_loss: -3.8933\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9467 - val_loss: -3.8929\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9468 - val_loss: -3.8927\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9468 - val_loss: -3.8934\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9469 - val_loss: -3.8928\n",
      "Epoch 33/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9470 - val_loss: -3.8933\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9471 - val_loss: -3.8930\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9472 - val_loss: -3.8931\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9472 - val_loss: -3.8928\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8930\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8936\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8930\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8924\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9475 - val_loss: -3.8931\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8928\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9476 - val_loss: -3.8927\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9475 - val_loss: -3.8929\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9477 - val_loss: -3.8929\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9477 - val_loss: -3.8925\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9479 - val_loss: -3.8930\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9479 - val_loss: -3.8929\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9480 - val_loss: -3.8918\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9481 - val_loss: -3.8930\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9483 - val_loss: -3.8931\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9483 - val_loss: -3.8929\n",
      "Epoch 53/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9485 - val_loss: -3.8926\n",
      "Epoch 54/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9485 - val_loss: -3.8924\n",
      "Epoch 55/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9486 - val_loss: -3.8930\n",
      "Epoch 56/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9484 - val_loss: -3.8927\n",
      "Epoch 57/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9487 - val_loss: -3.8920\n",
      "Epoch 58/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9487 - val_loss: -3.8928\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: -3.8942\n",
      "Iteration 2: Score: 3.894219398498535\n",
      "Both valid loss: -3.7270171642303467\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9449 - val_loss: -3.8934\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9450 - val_loss: -3.8937\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9451 - val_loss: -3.8932\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9452 - val_loss: -3.8936\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9454 - val_loss: -3.8936\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9455 - val_loss: -3.8931\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9456 - val_loss: -3.8935\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9456 - val_loss: -3.8935\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8933\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9458 - val_loss: -3.8937\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9459 - val_loss: -3.8936\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9459 - val_loss: -3.8933\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8934\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8934\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8936\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9461 - val_loss: -3.8931\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9463 - val_loss: -3.8932\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9464 - val_loss: -3.8931\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9464 - val_loss: -3.8933\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9466 - val_loss: -3.8930\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9466 - val_loss: -3.8934\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9465 - val_loss: -3.8930\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9467 - val_loss: -3.8935\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9468 - val_loss: -3.8931\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9469 - val_loss: -3.8931\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9470 - val_loss: -3.8933\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9471 - val_loss: -3.8926\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9471 - val_loss: -3.8931\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8932\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8931\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8933\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9474 - val_loss: -3.8931\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9475 - val_loss: -3.8925\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9475 - val_loss: -3.8932\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9476 - val_loss: -3.8932\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9477 - val_loss: -3.8922\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9478 - val_loss: -3.8928\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9478 - val_loss: -3.8928\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9479 - val_loss: -3.8921\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9479 - val_loss: -3.8927\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9481 - val_loss: -3.8928\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9482 - val_loss: -3.8928\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9481 - val_loss: -3.8925\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9482 - val_loss: -3.8927\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9482 - val_loss: -3.8917\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9484 - val_loss: -3.8929\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9483 - val_loss: -3.8920\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9484 - val_loss: -3.8925\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9485 - val_loss: -3.8927\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9487 - val_loss: -3.8924\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9486 - val_loss: -3.8922\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9489 - val_loss: -3.8925\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: -3.8937\n",
      "Iteration 3: Score: 3.893696117401123\n",
      "Performing final fit...\n",
      "Both valid loss: -3.7270171642303467\n",
      "Lag loss: -3.957709789276123\n",
      "Lag loss: -3.957709789276123\n",
      "Lag loss: -3.957709789276123\n",
      "Lag loss: -3.957709789276123\n",
      "Lag loss: -3.957709789276123\n",
      "Lag loss: -3.957709789276123\n",
      "Lag loss: -3.957709789276123\n",
      "Lag loss: -3.957709789276123\n",
      "118428/118428 [==============================] - 0s 4us/sample - loss: -3.7894\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/100\n",
      "450000/450000 [==============================] - 5s 11us/sample - loss: -6.0448e-05 - metric_VAMP: 2.4191 - val_loss: -6.1108e-05 - val_metric_VAMP: 2.4455\n",
      "Epoch 2/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1176e-05 - metric_VAMP: 2.4482 - val_loss: -6.1468e-05 - val_metric_VAMP: 2.4598\n",
      "Epoch 3/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1252e-05 - metric_VAMP: 2.4512 - val_loss: -6.1268e-05 - val_metric_VAMP: 2.4518\n",
      "Epoch 4/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1042e-05 - metric_VAMP: 2.4428 - val_loss: -6.0976e-05 - val_metric_VAMP: 2.4401\n",
      "Epoch 5/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0830e-05 - metric_VAMP: 2.4343 - val_loss: -6.0774e-05 - val_metric_VAMP: 2.4321\n",
      "Epoch 6/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0698e-05 - metric_VAMP: 2.4290 - val_loss: -6.0686e-05 - val_metric_VAMP: 2.4285\n",
      "Epoch 7/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0645e-05 - metric_VAMP: 2.4269 - val_loss: -6.0690e-05 - val_metric_VAMP: 2.4287\n",
      "Epoch 8/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0650e-05 - metric_VAMP: 2.4271 - val_loss: -6.0751e-05 - val_metric_VAMP: 2.4312\n",
      "Epoch 9/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0696e-05 - metric_VAMP: 2.4289 - val_loss: -6.0851e-05 - val_metric_VAMP: 2.4351\n",
      "Epoch 10/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0771e-05 - metric_VAMP: 2.4319 - val_loss: -6.0966e-05 - val_metric_VAMP: 2.4397\n",
      "Epoch 11/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0865e-05 - metric_VAMP: 2.4357 - val_loss: -6.1088e-05 - val_metric_VAMP: 2.4446\n",
      "Epoch 12/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.0963e-05 - metric_VAMP: 2.4396 - val_loss: -6.1221e-05 - val_metric_VAMP: 2.4499\n",
      "Epoch 13/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1073e-05 - metric_VAMP: 2.4440 - val_loss: -6.1345e-05 - val_metric_VAMP: 2.4549\n",
      "Epoch 14/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1183e-05 - metric_VAMP: 2.4484 - val_loss: -6.1466e-05 - val_metric_VAMP: 2.4597\n",
      "Epoch 15/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1296e-05 - metric_VAMP: 2.4529 - val_loss: -6.1575e-05 - val_metric_VAMP: 2.4641\n",
      "Epoch 16/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1406e-05 - metric_VAMP: 2.4574 - val_loss: -6.1679e-05 - val_metric_VAMP: 2.4683\n",
      "Epoch 17/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1514e-05 - metric_VAMP: 2.4617 - val_loss: -6.1778e-05 - val_metric_VAMP: 2.4722\n",
      "Epoch 18/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1619e-05 - metric_VAMP: 2.4659 - val_loss: -6.1869e-05 - val_metric_VAMP: 2.4758\n",
      "Epoch 19/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1720e-05 - metric_VAMP: 2.4699 - val_loss: -6.1959e-05 - val_metric_VAMP: 2.4795\n",
      "Epoch 20/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1818e-05 - metric_VAMP: 2.4738 - val_loss: -6.2043e-05 - val_metric_VAMP: 2.4828\n",
      "Epoch 21/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1907e-05 - metric_VAMP: 2.4774 - val_loss: -6.2113e-05 - val_metric_VAMP: 2.4857\n",
      "Epoch 22/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1994e-05 - metric_VAMP: 2.4809 - val_loss: -6.2184e-05 - val_metric_VAMP: 2.4885\n",
      "Epoch 23/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2074e-05 - metric_VAMP: 2.4841 - val_loss: -6.2247e-05 - val_metric_VAMP: 2.4910\n",
      "Epoch 24/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2152e-05 - metric_VAMP: 2.4872 - val_loss: -6.2302e-05 - val_metric_VAMP: 2.4932\n",
      "Epoch 25/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2217e-05 - metric_VAMP: 2.4898 - val_loss: -6.2350e-05 - val_metric_VAMP: 2.4951\n",
      "Epoch 26/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2279e-05 - metric_VAMP: 2.4923 - val_loss: -6.2393e-05 - val_metric_VAMP: 2.4968\n",
      "Epoch 27/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2332e-05 - metric_VAMP: 2.4944 - val_loss: -6.2437e-05 - val_metric_VAMP: 2.4986\n",
      "Epoch 28/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2377e-05 - metric_VAMP: 2.4962 - val_loss: -6.2463e-05 - val_metric_VAMP: 2.4996\n",
      "Epoch 29/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2420e-05 - metric_VAMP: 2.4979 - val_loss: -6.2500e-05 - val_metric_VAMP: 2.5011\n",
      "Epoch 30/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2457e-05 - metric_VAMP: 2.4994 - val_loss: -6.2525e-05 - val_metric_VAMP: 2.5021\n",
      "Epoch 31/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2491e-05 - metric_VAMP: 2.5008 - val_loss: -6.2552e-05 - val_metric_VAMP: 2.5032\n",
      "Epoch 32/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2524e-05 - metric_VAMP: 2.5021 - val_loss: -6.2574e-05 - val_metric_VAMP: 2.5041\n",
      "Epoch 33/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2543e-05 - metric_VAMP: 2.5033 - val_loss: -6.2597e-05 - val_metric_VAMP: 2.5050\n",
      "Epoch 34/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2586e-05 - metric_VAMP: 2.5045 - val_loss: -6.2623e-05 - val_metric_VAMP: 2.5060\n",
      "Epoch 35/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2615e-05 - metric_VAMP: 2.5057 - val_loss: -6.2644e-05 - val_metric_VAMP: 2.5069\n",
      "Epoch 36/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2646e-05 - metric_VAMP: 2.5069 - val_loss: -6.2668e-05 - val_metric_VAMP: 2.5078\n",
      "Epoch 37/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2675e-05 - metric_VAMP: 2.5081 - val_loss: -6.2693e-05 - val_metric_VAMP: 2.5088\n",
      "Epoch 38/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2704e-05 - metric_VAMP: 2.5093 - val_loss: -6.2712e-05 - val_metric_VAMP: 2.5096\n",
      "Epoch 39/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2721e-05 - metric_VAMP: 2.5104 - val_loss: -6.2735e-05 - val_metric_VAMP: 2.5105\n",
      "Epoch 40/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2763e-05 - metric_VAMP: 2.5116 - val_loss: -6.2757e-05 - val_metric_VAMP: 2.5114\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2793e-05 - metric_VAMP: 2.5169 - val_loss: -6.2781e-05 - val_metric_VAMP: 2.5123\n",
      "Epoch 42/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2823e-05 - metric_VAMP: 2.5140 - val_loss: -6.2806e-05 - val_metric_VAMP: 2.5133\n",
      "Epoch 43/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2853e-05 - metric_VAMP: 2.5152 - val_loss: -6.2828e-05 - val_metric_VAMP: 2.5142\n",
      "Epoch 44/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2881e-05 - metric_VAMP: 2.5164 - val_loss: -6.2852e-05 - val_metric_VAMP: 2.5152\n",
      "Epoch 45/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2909e-05 - metric_VAMP: 2.5175 - val_loss: -6.2878e-05 - val_metric_VAMP: 2.5162\n",
      "Epoch 46/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2940e-05 - metric_VAMP: 2.5187 - val_loss: -6.2902e-05 - val_metric_VAMP: 2.5172\n",
      "Epoch 47/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2967e-05 - metric_VAMP: 2.5198 - val_loss: -6.2924e-05 - val_metric_VAMP: 2.5181\n",
      "Epoch 48/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3022e-05 - metric_VAMP: 2.5210 - val_loss: -6.2949e-05 - val_metric_VAMP: 2.5191\n",
      "Epoch 49/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3025e-05 - metric_VAMP: 2.5221 - val_loss: -6.2533e-05 - val_metric_VAMP: 2.5319\n",
      "Epoch 50/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3053e-05 - metric_VAMP: 2.5232 - val_loss: -6.3002e-05 - val_metric_VAMP: 2.5211\n",
      "Epoch 51/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3081e-05 - metric_VAMP: 2.5243 - val_loss: -6.3025e-05 - val_metric_VAMP: 2.5221\n",
      "Epoch 52/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3110e-05 - metric_VAMP: 2.5255 - val_loss: -6.3052e-05 - val_metric_VAMP: 2.5232\n",
      "Epoch 53/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3139e-05 - metric_VAMP: 2.5266 - val_loss: -6.3080e-05 - val_metric_VAMP: 2.5243\n",
      "Epoch 54/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3166e-05 - metric_VAMP: 2.5277 - val_loss: -6.3105e-05 - val_metric_VAMP: 2.5253\n",
      "Epoch 55/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3289e-05 - metric_VAMP: 2.5288 - val_loss: -6.3131e-05 - val_metric_VAMP: 2.5264\n",
      "Epoch 56/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3222e-05 - metric_VAMP: 2.5300 - val_loss: -6.3159e-05 - val_metric_VAMP: 2.5275\n",
      "Epoch 57/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3280e-05 - metric_VAMP: 2.5310 - val_loss: -6.3186e-05 - val_metric_VAMP: 2.5286\n",
      "Epoch 58/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3275e-05 - metric_VAMP: 2.5321 - val_loss: -6.3212e-05 - val_metric_VAMP: 2.5296\n",
      "Epoch 59/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3280e-05 - metric_VAMP: 2.5332 - val_loss: -6.3241e-05 - val_metric_VAMP: 2.5308\n",
      "Epoch 60/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3484e-05 - metric_VAMP: 2.5342 - val_loss: -6.3262e-05 - val_metric_VAMP: 2.5316\n",
      "Epoch 61/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3354e-05 - metric_VAMP: 2.5352 - val_loss: -6.3286e-05 - val_metric_VAMP: 2.5325\n",
      "Epoch 62/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3280e-05 - metric_VAMP: 2.5402 - val_loss: -6.3312e-05 - val_metric_VAMP: 2.5336\n",
      "Epoch 63/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3404e-05 - metric_VAMP: 2.5373 - val_loss: -6.3334e-05 - val_metric_VAMP: 2.5345\n",
      "Epoch 64/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3435e-05 - metric_VAMP: 2.5496 - val_loss: -6.3355e-05 - val_metric_VAMP: 2.5353\n",
      "Epoch 65/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3450e-05 - metric_VAMP: 2.5391 - val_loss: -6.3382e-05 - val_metric_VAMP: 2.5364\n",
      "Epoch 66/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3499e-05 - metric_VAMP: 2.5474 - val_loss: -6.3403e-05 - val_metric_VAMP: 2.5372\n",
      "Epoch 67/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3496e-05 - metric_VAMP: 2.5409 - val_loss: -6.3423e-05 - val_metric_VAMP: 2.5380\n",
      "Epoch 68/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3516e-05 - metric_VAMP: 2.5417 - val_loss: -6.3444e-05 - val_metric_VAMP: 2.5388\n",
      "Epoch 69/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3467e-05 - metric_VAMP: 2.5480 - val_loss: -6.3462e-05 - val_metric_VAMP: 2.5396\n",
      "Epoch 70/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3623e-05 - metric_VAMP: 2.5435 - val_loss: -6.3484e-05 - val_metric_VAMP: 2.5405\n",
      "Epoch 71/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3685e-05 - metric_VAMP: 2.5443 - val_loss: -6.3489e-05 - val_metric_VAMP: 2.5413\n",
      "Epoch 72/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3610e-05 - metric_VAMP: 2.5451 - val_loss: -6.3522e-05 - val_metric_VAMP: 2.5420\n",
      "Epoch 73/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3620e-05 - metric_VAMP: 2.5459 - val_loss: -6.3542e-05 - val_metric_VAMP: 2.5428\n",
      "Epoch 74/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3640e-05 - metric_VAMP: 2.5467 - val_loss: -6.3563e-05 - val_metric_VAMP: 2.5436\n",
      "Epoch 75/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3659e-05 - metric_VAMP: 2.5474 - val_loss: -6.2411e-05 - val_metric_VAMP: 2.5443\n",
      "Epoch 76/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3722e-05 - metric_VAMP: 2.5482 - val_loss: -6.3596e-05 - val_metric_VAMP: 2.5449\n",
      "Epoch 77/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3613e-05 - metric_VAMP: 2.5534 - val_loss: -6.3616e-05 - val_metric_VAMP: 2.5457\n",
      "Epoch 78/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3660e-05 - metric_VAMP: 2.5567 - val_loss: -6.4104e-05 - val_metric_VAMP: 2.5465\n",
      "Epoch 79/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3794e-05 - metric_VAMP: 2.5526 - val_loss: -6.3650e-05 - val_metric_VAMP: 2.5471\n",
      "Epoch 80/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3714e-05 - metric_VAMP: 2.5512 - val_loss: -6.3668e-05 - val_metric_VAMP: 2.6043\n",
      "Epoch 81/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3767e-05 - metric_VAMP: 2.5542 - val_loss: -6.3686e-05 - val_metric_VAMP: 2.5485\n",
      "Epoch 82/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3789e-05 - metric_VAMP: 2.5526 - val_loss: -6.4056e-05 - val_metric_VAMP: 2.6171\n",
      "Epoch 83/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3793e-05 - metric_VAMP: 2.5533 - val_loss: -6.3719e-05 - val_metric_VAMP: 2.5498\n",
      "Epoch 84/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3835e-05 - metric_VAMP: 2.5541 - val_loss: -6.3735e-05 - val_metric_VAMP: 2.5505\n",
      "Epoch 85/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3806e-05 - metric_VAMP: 2.5611 - val_loss: -6.3753e-05 - val_metric_VAMP: 2.5512\n",
      "Epoch 86/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3859e-05 - metric_VAMP: 2.5555 - val_loss: -6.3768e-05 - val_metric_VAMP: 2.5518\n",
      "Epoch 87/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3920e-05 - metric_VAMP: 2.5639 - val_loss: -6.3337e-05 - val_metric_VAMP: 2.5524\n",
      "Epoch 88/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3854e-05 - metric_VAMP: 2.5568 - val_loss: -6.3798e-05 - val_metric_VAMP: 2.5530\n",
      "Epoch 89/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3912e-05 - metric_VAMP: 2.5576 - val_loss: -6.3816e-05 - val_metric_VAMP: 2.5537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3931e-05 - metric_VAMP: 2.5582 - val_loss: -6.3832e-05 - val_metric_VAMP: 2.5543\n",
      "Epoch 91/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3898e-05 - metric_VAMP: 2.5588 - val_loss: -6.4704e-05 - val_metric_VAMP: 2.5549\n",
      "Epoch 92/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3999e-05 - metric_VAMP: 2.5595 - val_loss: -6.3863e-05 - val_metric_VAMP: 2.5556\n",
      "Epoch 93/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.4023e-05 - metric_VAMP: 2.5602 - val_loss: -6.3877e-05 - val_metric_VAMP: 2.5562\n",
      "Epoch 94/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3898e-05 - metric_VAMP: 2.5609 - val_loss: -6.3891e-05 - val_metric_VAMP: 2.5567\n",
      "Epoch 95/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.4052e-05 - metric_VAMP: 2.5615 - val_loss: -6.3907e-05 - val_metric_VAMP: 2.5574\n",
      "Epoch 96/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.4112e-05 - metric_VAMP: 2.5622 - val_loss: -6.3923e-05 - val_metric_VAMP: 2.5580\n",
      "Epoch 97/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.3988e-05 - metric_VAMP: 2.5628 - val_loss: -6.3935e-05 - val_metric_VAMP: 2.5585\n",
      "Epoch 98/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.4029e-05 - metric_VAMP: 2.5681 - val_loss: -6.3951e-05 - val_metric_VAMP: 2.5591\n",
      "Epoch 99/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.4355e-05 - metric_VAMP: 2.5733 - val_loss: -6.3965e-05 - val_metric_VAMP: 2.5597\n",
      "Epoch 100/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.4025e-05 - metric_VAMP: 2.5647 - val_loss: -6.3977e-05 - val_metric_VAMP: 2.5602\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/100\n",
      "450000/450000 [==============================] - 5s 10us/sample - loss: -2.2724 - metric_VAMP: 2.6088 - val_loss: -2.3550 - val_metric_VAMP: 2.6565\n",
      "Epoch 2/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.4251 - metric_VAMP: 2.6963 - val_loss: -2.4757 - val_metric_VAMP: 2.7246\n",
      "Epoch 3/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.5174 - metric_VAMP: 2.7475 - val_loss: -2.5425 - val_metric_VAMP: 2.7613\n",
      "Epoch 4/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.5754 - metric_VAMP: 2.7792 - val_loss: -2.5971 - val_metric_VAMP: 2.7910\n",
      "Epoch 5/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.6215 - metric_VAMP: 2.8041 - val_loss: -2.6367 - val_metric_VAMP: 2.8123\n",
      "Epoch 6/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.6572 - metric_VAMP: 2.8232 - val_loss: -2.6672 - val_metric_VAMP: 2.8286\n",
      "Epoch 7/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.6886 - metric_VAMP: 2.8399 - val_loss: -2.6957 - val_metric_VAMP: 2.8437\n",
      "Epoch 8/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7143 - metric_VAMP: 2.8535 - val_loss: -2.7193 - val_metric_VAMP: 2.8561\n",
      "Epoch 9/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7357 - metric_VAMP: 2.8647 - val_loss: -2.7380 - val_metric_VAMP: 2.8660\n",
      "Epoch 10/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7535 - metric_VAMP: 2.8740 - val_loss: -2.7548 - val_metric_VAMP: 2.8747\n",
      "Epoch 11/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7689 - metric_VAMP: 2.8821 - val_loss: -2.7683 - val_metric_VAMP: 2.8818\n",
      "Epoch 12/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7821 - metric_VAMP: 2.8890 - val_loss: -2.7808 - val_metric_VAMP: 2.8883\n",
      "Epoch 13/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7939 - metric_VAMP: 2.8951 - val_loss: -2.7904 - val_metric_VAMP: 2.8932\n",
      "Epoch 14/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8045 - metric_VAMP: 2.9005 - val_loss: -2.7999 - val_metric_VAMP: 2.8981\n",
      "Epoch 15/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8140 - metric_VAMP: 2.9055 - val_loss: -2.8074 - val_metric_VAMP: 2.9020\n",
      "Epoch 16/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8234 - metric_VAMP: 2.9103 - val_loss: -2.8155 - val_metric_VAMP: 2.9062\n",
      "Epoch 17/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8322 - metric_VAMP: 2.9148 - val_loss: -2.8230 - val_metric_VAMP: 2.9101\n",
      "Epoch 18/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8408 - metric_VAMP: 2.9192 - val_loss: -2.8312 - val_metric_VAMP: 2.9143\n",
      "Epoch 19/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8488 - metric_VAMP: 2.9234 - val_loss: -2.8379 - val_metric_VAMP: 2.9178\n",
      "Epoch 20/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8561 - metric_VAMP: 2.9271 - val_loss: -2.8450 - val_metric_VAMP: 2.9214\n",
      "Epoch 21/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8630 - metric_VAMP: 2.9306 - val_loss: -2.8508 - val_metric_VAMP: 2.9244\n",
      "Epoch 22/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8697 - metric_VAMP: 2.9341 - val_loss: -2.8565 - val_metric_VAMP: 2.9273\n",
      "Epoch 23/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8760 - metric_VAMP: 2.9373 - val_loss: -2.8625 - val_metric_VAMP: 2.9304\n",
      "Epoch 24/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8814 - metric_VAMP: 2.9401 - val_loss: -2.8677 - val_metric_VAMP: 2.9330\n",
      "Epoch 25/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8869 - metric_VAMP: 2.9428 - val_loss: -2.8725 - val_metric_VAMP: 2.9355\n",
      "Epoch 26/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8920 - metric_VAMP: 2.9454 - val_loss: -2.8772 - val_metric_VAMP: 2.9379\n",
      "Epoch 27/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8965 - metric_VAMP: 2.9478 - val_loss: -2.8815 - val_metric_VAMP: 2.9401\n",
      "Epoch 28/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9008 - metric_VAMP: 2.9499 - val_loss: -2.8857 - val_metric_VAMP: 2.9422\n",
      "Epoch 29/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9049 - metric_VAMP: 2.9520 - val_loss: -2.8899 - val_metric_VAMP: 2.9444\n",
      "Epoch 30/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9088 - metric_VAMP: 2.9540 - val_loss: -2.8939 - val_metric_VAMP: 2.9464\n",
      "Epoch 31/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9125 - metric_VAMP: 2.9559 - val_loss: -2.8970 - val_metric_VAMP: 2.9480\n",
      "Epoch 32/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9157 - metric_VAMP: 2.9575 - val_loss: -2.9002 - val_metric_VAMP: 2.9496\n",
      "Epoch 33/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9188 - metric_VAMP: 2.9591 - val_loss: -2.9038 - val_metric_VAMP: 2.9515\n",
      "Epoch 34/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9220 - metric_VAMP: 2.9607 - val_loss: -2.9069 - val_metric_VAMP: 2.9531\n",
      "Epoch 35/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9245 - metric_VAMP: 2.9620 - val_loss: -2.9089 - val_metric_VAMP: 2.9541\n",
      "Epoch 36/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9271 - metric_VAMP: 2.9633 - val_loss: -2.9113 - val_metric_VAMP: 2.9553\n",
      "Epoch 37/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9299 - metric_VAMP: 2.9647 - val_loss: -2.9158 - val_metric_VAMP: 2.9576\n",
      "Epoch 38/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9326 - metric_VAMP: 2.9661 - val_loss: -2.9175 - val_metric_VAMP: 2.9584\n",
      "Epoch 39/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9350 - metric_VAMP: 2.9673 - val_loss: -2.9192 - val_metric_VAMP: 2.9593\n",
      "Epoch 40/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9371 - metric_VAMP: 2.9684 - val_loss: -2.9213 - val_metric_VAMP: 2.9604\n",
      "Epoch 41/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9392 - metric_VAMP: 2.9695 - val_loss: -2.9230 - val_metric_VAMP: 2.9612\n",
      "Epoch 42/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9411 - metric_VAMP: 2.9704 - val_loss: -2.9246 - val_metric_VAMP: 2.9620\n",
      "Epoch 43/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9428 - metric_VAMP: 2.9713 - val_loss: -2.9257 - val_metric_VAMP: 2.9626\n",
      "Epoch 44/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9448 - metric_VAMP: 2.9722 - val_loss: -2.9276 - val_metric_VAMP: 2.9636\n",
      "Epoch 45/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9464 - metric_VAMP: 2.9731 - val_loss: -2.9290 - val_metric_VAMP: 2.9643\n",
      "Epoch 46/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9481 - metric_VAMP: 2.9739 - val_loss: -2.9299 - val_metric_VAMP: 2.9647\n",
      "Epoch 47/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9495 - metric_VAMP: 2.9746 - val_loss: -2.9318 - val_metric_VAMP: 2.9657\n",
      "Epoch 48/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9514 - metric_VAMP: 2.9756 - val_loss: -2.9343 - val_metric_VAMP: 2.9669\n",
      "Epoch 49/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9529 - metric_VAMP: 2.9763 - val_loss: -2.9358 - val_metric_VAMP: 2.9677\n",
      "Epoch 50/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9544 - metric_VAMP: 2.9771 - val_loss: -2.9376 - val_metric_VAMP: 2.9686\n",
      "Epoch 51/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9558 - metric_VAMP: 2.9778 - val_loss: -2.9379 - val_metric_VAMP: 2.9688\n",
      "Epoch 52/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9571 - metric_VAMP: 2.9784 - val_loss: -2.9392 - val_metric_VAMP: 2.9695\n",
      "Epoch 53/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9583 - metric_VAMP: 2.9791 - val_loss: -2.9394 - val_metric_VAMP: 2.9696\n",
      "Epoch 54/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9592 - metric_VAMP: 2.9795 - val_loss: -2.9410 - val_metric_VAMP: 2.9704\n",
      "Epoch 55/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9600 - metric_VAMP: 2.9799 - val_loss: -2.9414 - val_metric_VAMP: 2.9705\n",
      "Epoch 56/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9612 - metric_VAMP: 2.9805 - val_loss: -2.9428 - val_metric_VAMP: 2.9712\n",
      "Epoch 57/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9619 - metric_VAMP: 2.9809 - val_loss: -2.9433 - val_metric_VAMP: 2.9715\n",
      "Epoch 58/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9626 - metric_VAMP: 2.9812 - val_loss: -2.9438 - val_metric_VAMP: 2.9718\n",
      "Epoch 59/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9633 - metric_VAMP: 2.9816 - val_loss: -2.9439 - val_metric_VAMP: 2.9718\n",
      "Epoch 60/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9637 - metric_VAMP: 2.9818 - val_loss: -2.9451 - val_metric_VAMP: 2.9724\n",
      "Epoch 61/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9644 - metric_VAMP: 2.9821 - val_loss: -2.9451 - val_metric_VAMP: 2.9724\n",
      "Epoch 62/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9651 - metric_VAMP: 2.9825 - val_loss: -2.9449 - val_metric_VAMP: 2.9723\n",
      "Epoch 63/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9659 - metric_VAMP: 2.9829 - val_loss: -2.9464 - val_metric_VAMP: 2.9731\n",
      "Epoch 64/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9663 - metric_VAMP: 2.9831 - val_loss: -2.9463 - val_metric_VAMP: 2.9730\n",
      "Epoch 65/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9669 - metric_VAMP: 2.9834 - val_loss: -2.9461 - val_metric_VAMP: 2.9729\n",
      "Epoch 66/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9674 - metric_VAMP: 2.9836 - val_loss: -2.9459 - val_metric_VAMP: 2.9728\n",
      "Epoch 67/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9678 - metric_VAMP: 2.9838 - val_loss: -2.9466 - val_metric_VAMP: 2.9732\n",
      "Epoch 68/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9683 - metric_VAMP: 2.9841 - val_loss: -2.9465 - val_metric_VAMP: 2.9731\n",
      "Epoch 69/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9693 - metric_VAMP: 2.9846 - val_loss: -2.9469 - val_metric_VAMP: 2.9733\n",
      "Epoch 70/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9695 - metric_VAMP: 2.9847 - val_loss: -2.9485 - val_metric_VAMP: 2.9742\n",
      "Epoch 71/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9703 - metric_VAMP: 2.9851 - val_loss: -2.9489 - val_metric_VAMP: 2.9744\n",
      "Epoch 72/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9706 - metric_VAMP: 2.9853 - val_loss: -2.9488 - val_metric_VAMP: 2.9743\n",
      "Epoch 73/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9712 - metric_VAMP: 2.9856 - val_loss: -2.9495 - val_metric_VAMP: 2.9746\n",
      "Epoch 74/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9715 - metric_VAMP: 2.9857 - val_loss: -2.9501 - val_metric_VAMP: 2.9749\n",
      "Epoch 75/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9718 - metric_VAMP: 2.9859 - val_loss: -2.9506 - val_metric_VAMP: 2.9752\n",
      "Epoch 76/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9724 - metric_VAMP: 2.9862 - val_loss: -2.9501 - val_metric_VAMP: 2.9749\n",
      "Epoch 77/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9728 - metric_VAMP: 2.9864 - val_loss: -2.9500 - val_metric_VAMP: 2.9749\n",
      "Epoch 78/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9731 - metric_VAMP: 2.9865 - val_loss: -2.9509 - val_metric_VAMP: 2.9754\n",
      "Epoch 79/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9737 - metric_VAMP: 2.9868 - val_loss: -2.9512 - val_metric_VAMP: 2.9755\n",
      "Epoch 80/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9738 - metric_VAMP: 2.9869 - val_loss: -2.9521 - val_metric_VAMP: 2.9759\n",
      "Epoch 81/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9745 - metric_VAMP: 2.9872 - val_loss: -2.9519 - val_metric_VAMP: 2.9759\n",
      "Epoch 82/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9746 - metric_VAMP: 2.9873 - val_loss: -2.9509 - val_metric_VAMP: 2.9754\n",
      "Epoch 83/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9747 - metric_VAMP: 2.9873 - val_loss: -2.9504 - val_metric_VAMP: 2.9751\n",
      "Epoch 84/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9747 - metric_VAMP: 2.9873 - val_loss: -2.9521 - val_metric_VAMP: 2.9760\n",
      "Epoch 85/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9744 - metric_VAMP: 2.9872 - val_loss: -2.9517 - val_metric_VAMP: 2.9757\n",
      "Epoch 86/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9750 - metric_VAMP: 2.9875 - val_loss: -2.9519 - val_metric_VAMP: 2.9759\n",
      "Epoch 87/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9751 - metric_VAMP: 2.9875 - val_loss: -2.9516 - val_metric_VAMP: 2.9757\n",
      "Epoch 88/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9752 - metric_VAMP: 2.9876 - val_loss: -2.9516 - val_metric_VAMP: 2.9757\n",
      "Epoch 89/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9760 - metric_VAMP: 2.9880 - val_loss: -2.9523 - val_metric_VAMP: 2.9761\n",
      "Epoch 90/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9769 - metric_VAMP: 2.9884 - val_loss: -2.9522 - val_metric_VAMP: 2.9760\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9772 - metric_VAMP: 2.9886 - val_loss: -2.9514 - val_metric_VAMP: 2.9756\n",
      "Epoch 92/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9774 - metric_VAMP: 2.9887 - val_loss: -2.9526 - val_metric_VAMP: 2.9762\n",
      "Epoch 93/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9780 - metric_VAMP: 2.9890 - val_loss: -2.9523 - val_metric_VAMP: 2.9761\n",
      "Epoch 94/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9781 - metric_VAMP: 2.9891 - val_loss: -2.9525 - val_metric_VAMP: 2.9761\n",
      "Epoch 95/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9780 - metric_VAMP: 2.9890 - val_loss: -2.9529 - val_metric_VAMP: 2.9763\n",
      "Epoch 96/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9785 - metric_VAMP: 2.9892 - val_loss: -2.9527 - val_metric_VAMP: 2.9763\n",
      "Epoch 97/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9785 - metric_VAMP: 2.9892 - val_loss: -2.9522 - val_metric_VAMP: 2.9760\n",
      "Epoch 98/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9790 - metric_VAMP: 2.9895 - val_loss: -2.9529 - val_metric_VAMP: 2.9764\n",
      "Epoch 99/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9794 - metric_VAMP: 2.9897 - val_loss: -2.9520 - val_metric_VAMP: 2.9759\n",
      "Epoch 100/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9796 - metric_VAMP: 2.9898 - val_loss: -2.9531 - val_metric_VAMP: 2.9765\n",
      "Setting initial auxiliary weights...\n",
      "Training auxiliary network...\n",
      "Both valid loss: -3.9331746101379395\n",
      "Training full network...\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 8s 18us/sample - loss: -3.9630 - val_loss: -3.9390\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9664 - val_loss: -3.9410\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9682 - val_loss: -3.9421\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9693 - val_loss: -3.9426\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9701 - val_loss: -3.9429\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9708 - val_loss: -3.9438\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9714 - val_loss: -3.9443\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9718 - val_loss: -3.9448\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9722 - val_loss: -3.9445\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9726 - val_loss: -3.9454\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9731 - val_loss: -3.9447\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9733 - val_loss: -3.9453\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9736 - val_loss: -3.9449\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9739 - val_loss: -3.9453\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9742 - val_loss: -3.9453\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9745 - val_loss: -3.9453\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9746 - val_loss: -3.9454\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9748 - val_loss: -3.9451\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9751 - val_loss: -3.9459\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9752 - val_loss: -3.9454\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9754 - val_loss: -3.9450\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9755 - val_loss: -3.9457\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9757 - val_loss: -3.9458\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9759 - val_loss: -3.9451\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9760 - val_loss: -3.9456\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9760 - val_loss: -3.9453\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9763 - val_loss: -3.9455\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9764 - val_loss: -3.9454\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9766 - val_loss: -3.9455\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9767 - val_loss: -3.9456\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9768 - val_loss: -3.9456\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9769 - val_loss: -3.9455\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9771 - val_loss: -3.9453\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9772 - val_loss: -3.9455\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9773 - val_loss: -3.9450\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9773 - val_loss: -3.9449\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9776 - val_loss: -3.9450\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9776 - val_loss: -3.9451\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9777 - val_loss: -3.9452\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9778 - val_loss: -3.9450\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9778 - val_loss: -3.9442\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9778 - val_loss: -3.9451\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9780 - val_loss: -3.9448\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9781 - val_loss: -3.9446\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9782 - val_loss: -3.9447\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9784 - val_loss: -3.9449\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9786 - val_loss: -3.9448\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9785 - val_loss: -3.9450\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9787 - val_loss: -3.9442\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9789 - val_loss: -3.9445\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9789 - val_loss: -3.9442\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9789 - val_loss: -3.9451\n",
      "Epoch 53/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9789 - val_loss: -3.9445\n",
      "Epoch 54/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9790 - val_loss: -3.9441\n",
      "Epoch 55/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9790 - val_loss: -3.9439\n",
      "Epoch 56/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9788 - val_loss: -3.9441\n",
      "Epoch 57/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9792 - val_loss: -3.9439\n",
      "Epoch 58/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9793 - val_loss: -3.9444\n",
      "Epoch 59/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9794 - val_loss: -3.9442\n",
      "Epoch 60/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9797 - val_loss: -3.9431\n",
      "Epoch 61/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9797 - val_loss: -3.9444\n",
      "Epoch 62/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9797 - val_loss: -3.9437\n",
      "Epoch 63/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9799 - val_loss: -3.9438\n",
      "Epoch 64/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9800 - val_loss: -3.9443\n",
      "Epoch 65/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9800 - val_loss: -3.9440\n",
      "Epoch 66/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9800 - val_loss: -3.9438\n",
      "Epoch 67/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9801 - val_loss: -3.9437\n",
      "Epoch 68/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9802 - val_loss: -3.9435\n",
      "Epoch 69/10000\n",
      "450000/450000 [==============================] - 3s 7us/sample - loss: -3.9802 - val_loss: -3.9433\n",
      "Setting up auxiliary training loop...\n",
      "Both valid loss: -3.9331746101379395\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 8s 17us/sample - loss: -3.9747 - val_loss: -3.9456\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9751 - val_loss: -3.9457\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9753 - val_loss: -3.9452\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9756 - val_loss: -3.9457\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9758 - val_loss: -3.9454\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9759 - val_loss: -3.9460\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9762 - val_loss: -3.9462\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9763 - val_loss: -3.9455\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9764 - val_loss: -3.9458\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9766 - val_loss: -3.9456\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9766 - val_loss: -3.9458\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9769 - val_loss: -3.9458\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9770 - val_loss: -3.9455\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9770 - val_loss: -3.9460\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9772 - val_loss: -3.9454\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9773 - val_loss: -3.9452\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9773 - val_loss: -3.9457\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9774 - val_loss: -3.9455\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9776 - val_loss: -3.9450\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9778 - val_loss: -3.9451\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9778 - val_loss: -3.9452\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9780 - val_loss: -3.9450\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9780 - val_loss: -3.9452\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9781 - val_loss: -3.9446\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9782 - val_loss: -3.9451\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9783 - val_loss: -3.9448\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9782 - val_loss: -3.9450\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9785 - val_loss: -3.9451\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9786 - val_loss: -3.9445\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9787 - val_loss: -3.9449\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9788 - val_loss: -3.9449\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9789 - val_loss: -3.9448\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9789 - val_loss: -3.9446\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9790 - val_loss: -3.9447\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9791 - val_loss: -3.9442\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9792 - val_loss: -3.9446\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9792 - val_loss: -3.9447\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9794 - val_loss: -3.9441\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9795 - val_loss: -3.9445\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9795 - val_loss: -3.9443\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9795 - val_loss: -3.9439\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9796 - val_loss: -3.9441\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9796 - val_loss: -3.9438\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9797 - val_loss: -3.9438\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9798 - val_loss: -3.9440\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9800 - val_loss: -3.9436\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9800 - val_loss: -3.9443\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9800 - val_loss: -3.9438\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9802 - val_loss: -3.9437\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9802 - val_loss: -3.9435\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9802 - val_loss: -3.9435\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9803 - val_loss: -3.9439\n",
      "Epoch 53/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9804 - val_loss: -3.9439\n",
      "Epoch 54/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9804 - val_loss: -3.9431\n",
      "Epoch 55/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9806 - val_loss: -3.9436\n",
      "Epoch 56/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9807 - val_loss: -3.9431\n",
      "Epoch 57/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9806 - val_loss: -3.9433\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: -3.9459\n",
      "Iteration 0: Score: 3.945941686630249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both valid loss: -3.9331746101379395\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9762 - val_loss: -3.9457\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9765 - val_loss: -3.9455\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9765 - val_loss: -3.9456\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9766 - val_loss: -3.9452\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9767 - val_loss: -3.9455\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9768 - val_loss: -3.9448\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9770 - val_loss: -3.9455\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9770 - val_loss: -3.9457\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9772 - val_loss: -3.9455\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9772 - val_loss: -3.9452\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9774 - val_loss: -3.9452\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9774 - val_loss: -3.9447\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9774 - val_loss: -3.9450\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9777 - val_loss: -3.9452\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9779 - val_loss: -3.9453\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9780 - val_loss: -3.9450\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9782 - val_loss: -3.9452\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9783 - val_loss: -3.9451\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9783 - val_loss: -3.9451\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9786 - val_loss: -3.9450\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9786 - val_loss: -3.9448\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9787 - val_loss: -3.9447\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9787 - val_loss: -3.9449\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9788 - val_loss: -3.9446\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9789 - val_loss: -3.9446\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9790 - val_loss: -3.9450\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9791 - val_loss: -3.9443\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9792 - val_loss: -3.9446\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9793 - val_loss: -3.9442\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9793 - val_loss: -3.9442\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9795 - val_loss: -3.9443\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9796 - val_loss: -3.9443\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9794 - val_loss: -3.9443\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9795 - val_loss: -3.9445\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9795 - val_loss: -3.9438\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9795 - val_loss: -3.9441\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9798 - val_loss: -3.9438\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9798 - val_loss: -3.9441\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9800 - val_loss: -3.9439\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9801 - val_loss: -3.9438\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9800 - val_loss: -3.9436\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9800 - val_loss: -3.9439\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9802 - val_loss: -3.9436\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9803 - val_loss: -3.9439\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9803 - val_loss: -3.9433\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9804 - val_loss: -3.9438\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9804 - val_loss: -3.9431\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9805 - val_loss: -3.9428\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9804 - val_loss: -3.9425\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9802 - val_loss: -3.9427\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9803 - val_loss: -3.9433\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9803 - val_loss: -3.9429\n",
      "Epoch 53/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9805 - val_loss: -3.9428\n",
      "Epoch 54/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9806 - val_loss: -3.9427\n",
      "Epoch 55/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9807 - val_loss: -3.9432\n",
      "Epoch 56/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9809 - val_loss: -3.9428\n",
      "Epoch 57/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9810 - val_loss: -3.9427\n",
      "Epoch 58/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9812 - val_loss: -3.9427\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: -3.9455\n",
      "Iteration 1: Score: 3.945523166656494\n",
      "Performing final fit...\n",
      "Both valid loss: -3.9331746101379395\n",
      "Lag loss: -3.981832265853882\n",
      "Lag loss: -3.981832265853882\n",
      "Lag loss: -3.981832265853882\n",
      "Lag loss: -3.981832265853882\n",
      "Lag loss: -3.981832265853882\n",
      "Lag loss: -3.981832265853882\n",
      "Lag loss: -3.981832265853882\n",
      "Lag loss: -3.981832265853882\n",
      "118428/118428 [==============================] - 0s 4us/sample - loss: -3.8945\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/100\n",
      "450000/450000 [==============================] - 6s 13us/sample - loss: -6.0028e-05 - metric_VAMP: 2.4023 - val_loss: -6.0954e-05 - val_metric_VAMP: 2.4392\n",
      "Epoch 2/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1519e-05 - metric_VAMP: 2.4619 - val_loss: -6.1903e-05 - val_metric_VAMP: 2.4772\n",
      "Epoch 3/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2081e-05 - metric_VAMP: 2.4843 - val_loss: -6.2091e-05 - val_metric_VAMP: 2.4847\n",
      "Epoch 4/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2163e-05 - metric_VAMP: 2.4876 - val_loss: -6.2060e-05 - val_metric_VAMP: 2.4835\n",
      "Epoch 5/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2107e-05 - metric_VAMP: 2.4854 - val_loss: -6.1997e-05 - val_metric_VAMP: 2.4810\n",
      "Epoch 6/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2027e-05 - metric_VAMP: 2.4822 - val_loss: -6.1918e-05 - val_metric_VAMP: 2.4778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1957e-05 - metric_VAMP: 2.4794 - val_loss: -6.1838e-05 - val_metric_VAMP: 2.4746\n",
      "Epoch 8/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1887e-05 - metric_VAMP: 2.4766 - val_loss: -6.1767e-05 - val_metric_VAMP: 2.4718\n",
      "Epoch 9/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1829e-05 - metric_VAMP: 2.4743 - val_loss: -6.1722e-05 - val_metric_VAMP: 2.4700\n",
      "Epoch 10/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1799e-05 - metric_VAMP: 2.4731 - val_loss: -6.1703e-05 - val_metric_VAMP: 2.4692\n",
      "Epoch 11/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1784e-05 - metric_VAMP: 2.4725 - val_loss: -6.1704e-05 - val_metric_VAMP: 2.4693\n",
      "Epoch 12/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1789e-05 - metric_VAMP: 2.4727 - val_loss: -6.1726e-05 - val_metric_VAMP: 2.4701\n",
      "Epoch 13/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1807e-05 - metric_VAMP: 2.4734 - val_loss: -6.1759e-05 - val_metric_VAMP: 2.4715\n",
      "Epoch 14/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1831e-05 - metric_VAMP: 2.4743 - val_loss: -6.1791e-05 - val_metric_VAMP: 2.4727\n",
      "Epoch 15/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1861e-05 - metric_VAMP: 2.4755 - val_loss: -6.1824e-05 - val_metric_VAMP: 2.4741\n",
      "Epoch 16/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1889e-05 - metric_VAMP: 2.4767 - val_loss: -6.1853e-05 - val_metric_VAMP: 2.4752\n",
      "Epoch 17/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1922e-05 - metric_VAMP: 2.4780 - val_loss: -6.1884e-05 - val_metric_VAMP: 2.4764\n",
      "Epoch 18/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1953e-05 - metric_VAMP: 2.4792 - val_loss: -6.1913e-05 - val_metric_VAMP: 2.4776\n",
      "Epoch 19/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.1986e-05 - metric_VAMP: 2.4805 - val_loss: -6.1942e-05 - val_metric_VAMP: 2.4788\n",
      "Epoch 20/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2020e-05 - metric_VAMP: 2.4819 - val_loss: -6.1969e-05 - val_metric_VAMP: 2.4798\n",
      "Epoch 21/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2055e-05 - metric_VAMP: 2.4833 - val_loss: -6.1996e-05 - val_metric_VAMP: 2.4809\n",
      "Epoch 22/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -6.2088e-05 - metric_VAMP: 2.4846 - val_loss: -6.2026e-05 - val_metric_VAMP: 2.4821\n",
      "Epoch 23/100\n",
      "450000/450000 [==============================] - 4s 10us/sample - loss: -6.2124e-05 - metric_VAMP: 2.4861 - val_loss: -6.2054e-05 - val_metric_VAMP: 2.4833\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/100\n",
      "450000/450000 [==============================] - 5s 12us/sample - loss: -2.3248 - metric_VAMP: 2.6399 - val_loss: -2.4695 - val_metric_VAMP: 2.7217\n",
      "Epoch 2/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.5208 - metric_VAMP: 2.7498 - val_loss: -2.5696 - val_metric_VAMP: 2.7764\n",
      "Epoch 3/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.5989 - metric_VAMP: 2.7922 - val_loss: -2.6291 - val_metric_VAMP: 2.8084\n",
      "Epoch 4/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.6485 - metric_VAMP: 2.8187 - val_loss: -2.6670 - val_metric_VAMP: 2.8286\n",
      "Epoch 5/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.6811 - metric_VAMP: 2.8360 - val_loss: -2.6971 - val_metric_VAMP: 2.8445\n",
      "Epoch 6/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7087 - metric_VAMP: 2.8506 - val_loss: -2.7185 - val_metric_VAMP: 2.8557\n",
      "Epoch 7/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7293 - metric_VAMP: 2.8614 - val_loss: -2.7361 - val_metric_VAMP: 2.8650\n",
      "Epoch 8/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7462 - metric_VAMP: 2.8703 - val_loss: -2.7501 - val_metric_VAMP: 2.8723\n",
      "Epoch 9/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7594 - metric_VAMP: 2.8771 - val_loss: -2.7623 - val_metric_VAMP: 2.8787\n",
      "Epoch 10/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7706 - metric_VAMP: 2.8830 - val_loss: -2.7720 - val_metric_VAMP: 2.8837\n",
      "Epoch 11/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7801 - metric_VAMP: 2.8879 - val_loss: -2.7805 - val_metric_VAMP: 2.8881\n",
      "Epoch 12/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7888 - metric_VAMP: 2.8924 - val_loss: -2.7884 - val_metric_VAMP: 2.8922\n",
      "Epoch 13/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.7963 - metric_VAMP: 2.8963 - val_loss: -2.7952 - val_metric_VAMP: 2.8957\n",
      "Epoch 14/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8033 - metric_VAMP: 2.8999 - val_loss: -2.8020 - val_metric_VAMP: 2.8992\n",
      "Epoch 15/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8097 - metric_VAMP: 2.9032 - val_loss: -2.8079 - val_metric_VAMP: 2.9023\n",
      "Epoch 16/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8157 - metric_VAMP: 2.9063 - val_loss: -2.8138 - val_metric_VAMP: 2.9053\n",
      "Epoch 17/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8217 - metric_VAMP: 2.9094 - val_loss: -2.8197 - val_metric_VAMP: 2.9084\n",
      "Epoch 18/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8274 - metric_VAMP: 2.9123 - val_loss: -2.8256 - val_metric_VAMP: 2.9114\n",
      "Epoch 19/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8331 - metric_VAMP: 2.9153 - val_loss: -2.8315 - val_metric_VAMP: 2.9144\n",
      "Epoch 20/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8388 - metric_VAMP: 2.9181 - val_loss: -2.8372 - val_metric_VAMP: 2.9173\n",
      "Epoch 21/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8448 - metric_VAMP: 2.9212 - val_loss: -2.8432 - val_metric_VAMP: 2.9204\n",
      "Epoch 22/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8508 - metric_VAMP: 2.9243 - val_loss: -2.8489 - val_metric_VAMP: 2.9233\n",
      "Epoch 23/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8569 - metric_VAMP: 2.9274 - val_loss: -2.8549 - val_metric_VAMP: 2.9264\n",
      "Epoch 24/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8631 - metric_VAMP: 2.9306 - val_loss: -2.8602 - val_metric_VAMP: 2.9291\n",
      "Epoch 25/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8692 - metric_VAMP: 2.9337 - val_loss: -2.8651 - val_metric_VAMP: 2.9316\n",
      "Epoch 26/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8747 - metric_VAMP: 2.9365 - val_loss: -2.8710 - val_metric_VAMP: 2.9346\n",
      "Epoch 27/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8808 - metric_VAMP: 2.9396 - val_loss: -2.8753 - val_metric_VAMP: 2.9368\n",
      "Epoch 28/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8867 - metric_VAMP: 2.9427 - val_loss: -2.8816 - val_metric_VAMP: 2.9400\n",
      "Epoch 29/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8922 - metric_VAMP: 2.9455 - val_loss: -2.8866 - val_metric_VAMP: 2.9426\n",
      "Epoch 30/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.8979 - metric_VAMP: 2.9484 - val_loss: -2.8913 - val_metric_VAMP: 2.9450\n",
      "Epoch 31/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9032 - metric_VAMP: 2.9511 - val_loss: -2.8960 - val_metric_VAMP: 2.9474\n",
      "Epoch 32/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9088 - metric_VAMP: 2.9540 - val_loss: -2.9009 - val_metric_VAMP: 2.9499\n",
      "Epoch 33/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9148 - metric_VAMP: 2.9570 - val_loss: -2.9067 - val_metric_VAMP: 2.9529\n",
      "Epoch 34/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9203 - metric_VAMP: 2.9598 - val_loss: -2.9120 - val_metric_VAMP: 2.9556\n",
      "Epoch 35/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9253 - metric_VAMP: 2.9623 - val_loss: -2.9157 - val_metric_VAMP: 2.9575\n",
      "Epoch 36/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9302 - metric_VAMP: 2.9648 - val_loss: -2.9206 - val_metric_VAMP: 2.9600\n",
      "Epoch 37/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9348 - metric_VAMP: 2.9672 - val_loss: -2.9244 - val_metric_VAMP: 2.9619\n",
      "Epoch 38/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9388 - metric_VAMP: 2.9692 - val_loss: -2.9273 - val_metric_VAMP: 2.9634\n",
      "Epoch 39/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9423 - metric_VAMP: 2.9710 - val_loss: -2.9290 - val_metric_VAMP: 2.9642\n",
      "Epoch 40/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9454 - metric_VAMP: 2.9725 - val_loss: -2.9336 - val_metric_VAMP: 2.9666\n",
      "Epoch 41/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9484 - metric_VAMP: 2.9740 - val_loss: -2.9357 - val_metric_VAMP: 2.9676\n",
      "Epoch 42/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9509 - metric_VAMP: 2.9753 - val_loss: -2.9373 - val_metric_VAMP: 2.9684\n",
      "Epoch 43/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9529 - metric_VAMP: 2.9763 - val_loss: -2.9388 - val_metric_VAMP: 2.9692\n",
      "Epoch 44/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9549 - metric_VAMP: 2.9773 - val_loss: -2.9406 - val_metric_VAMP: 2.9701\n",
      "Epoch 45/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9566 - metric_VAMP: 2.9782 - val_loss: -2.9420 - val_metric_VAMP: 2.9708\n",
      "Epoch 46/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9581 - metric_VAMP: 2.9790 - val_loss: -2.9424 - val_metric_VAMP: 2.9710\n",
      "Epoch 47/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9585 - metric_VAMP: 2.9792 - val_loss: -2.9432 - val_metric_VAMP: 2.9714\n",
      "Epoch 48/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9600 - metric_VAMP: 2.9799 - val_loss: -2.9448 - val_metric_VAMP: 2.9722\n",
      "Epoch 49/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9617 - metric_VAMP: 2.9807 - val_loss: -2.9465 - val_metric_VAMP: 2.9731\n",
      "Epoch 50/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9629 - metric_VAMP: 2.9814 - val_loss: -2.9481 - val_metric_VAMP: 2.9739\n",
      "Epoch 51/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9641 - metric_VAMP: 2.9820 - val_loss: -2.9493 - val_metric_VAMP: 2.9745\n",
      "Epoch 52/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9652 - metric_VAMP: 2.9825 - val_loss: -2.9502 - val_metric_VAMP: 2.9750\n",
      "Epoch 53/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9663 - metric_VAMP: 2.9831 - val_loss: -2.9506 - val_metric_VAMP: 2.9752\n",
      "Epoch 54/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9669 - metric_VAMP: 2.9834 - val_loss: -2.9516 - val_metric_VAMP: 2.9757\n",
      "Epoch 55/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9679 - metric_VAMP: 2.9839 - val_loss: -2.9531 - val_metric_VAMP: 2.9764\n",
      "Epoch 56/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9688 - metric_VAMP: 2.9844 - val_loss: -2.9536 - val_metric_VAMP: 2.9767\n",
      "Epoch 57/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9693 - metric_VAMP: 2.9846 - val_loss: -2.9543 - val_metric_VAMP: 2.9771\n",
      "Epoch 58/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9702 - metric_VAMP: 2.9851 - val_loss: -2.9543 - val_metric_VAMP: 2.9770\n",
      "Epoch 59/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9710 - metric_VAMP: 2.9854 - val_loss: -2.9551 - val_metric_VAMP: 2.9775\n",
      "Epoch 60/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9719 - metric_VAMP: 2.9859 - val_loss: -2.9561 - val_metric_VAMP: 2.9780\n",
      "Epoch 61/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9727 - metric_VAMP: 2.9863 - val_loss: -2.9564 - val_metric_VAMP: 2.9781\n",
      "Epoch 62/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9731 - metric_VAMP: 2.9865 - val_loss: -2.9565 - val_metric_VAMP: 2.9782\n",
      "Epoch 63/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9739 - metric_VAMP: 2.9869 - val_loss: -2.9580 - val_metric_VAMP: 2.9789\n",
      "Epoch 64/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9749 - metric_VAMP: 2.9874 - val_loss: -2.9592 - val_metric_VAMP: 2.9795\n",
      "Epoch 65/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9754 - metric_VAMP: 2.9877 - val_loss: -2.9582 - val_metric_VAMP: 2.9790\n",
      "Epoch 66/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9760 - metric_VAMP: 2.9880 - val_loss: -2.9592 - val_metric_VAMP: 2.9795\n",
      "Epoch 67/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9765 - metric_VAMP: 2.9882 - val_loss: -2.9573 - val_metric_VAMP: 2.9785\n",
      "Epoch 68/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9768 - metric_VAMP: 2.9884 - val_loss: -2.9602 - val_metric_VAMP: 2.9800\n",
      "Epoch 69/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9777 - metric_VAMP: 2.9888 - val_loss: -2.9601 - val_metric_VAMP: 2.9800\n",
      "Epoch 70/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9782 - metric_VAMP: 2.9891 - val_loss: -2.9610 - val_metric_VAMP: 2.9804\n",
      "Epoch 71/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9785 - metric_VAMP: 2.9892 - val_loss: -2.9602 - val_metric_VAMP: 2.9800\n",
      "Epoch 72/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9791 - metric_VAMP: 2.9895 - val_loss: -2.9615 - val_metric_VAMP: 2.9807\n",
      "Epoch 73/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9796 - metric_VAMP: 2.9898 - val_loss: -2.9618 - val_metric_VAMP: 2.9808\n",
      "Epoch 74/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9798 - metric_VAMP: 2.9899 - val_loss: -2.9619 - val_metric_VAMP: 2.9809\n",
      "Epoch 75/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9803 - metric_VAMP: 2.9901 - val_loss: -2.9621 - val_metric_VAMP: 2.9810\n",
      "Epoch 76/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9807 - metric_VAMP: 2.9903 - val_loss: -2.9617 - val_metric_VAMP: 2.9808\n",
      "Epoch 77/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9812 - metric_VAMP: 2.9906 - val_loss: -2.9625 - val_metric_VAMP: 2.9812\n",
      "Epoch 78/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9815 - metric_VAMP: 2.9907 - val_loss: -2.9626 - val_metric_VAMP: 2.9812\n",
      "Epoch 79/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9818 - metric_VAMP: 2.9909 - val_loss: -2.9631 - val_metric_VAMP: 2.9815\n",
      "Epoch 80/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9819 - metric_VAMP: 2.9909 - val_loss: -2.9621 - val_metric_VAMP: 2.9810\n",
      "Epoch 81/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9821 - metric_VAMP: 2.9910 - val_loss: -2.9619 - val_metric_VAMP: 2.9809\n",
      "Epoch 82/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9824 - metric_VAMP: 2.9912 - val_loss: -2.9625 - val_metric_VAMP: 2.9812\n",
      "Epoch 83/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9825 - metric_VAMP: 2.9912 - val_loss: -2.9626 - val_metric_VAMP: 2.9812\n",
      "Epoch 84/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9826 - metric_VAMP: 2.9913 - val_loss: -2.9622 - val_metric_VAMP: 2.9810\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9830 - metric_VAMP: 2.9915 - val_loss: -2.9628 - val_metric_VAMP: 2.9814\n",
      "Epoch 86/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9831 - metric_VAMP: 2.9915 - val_loss: -2.9620 - val_metric_VAMP: 2.9809\n",
      "Epoch 87/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9831 - metric_VAMP: 2.9915 - val_loss: -2.9623 - val_metric_VAMP: 2.9811\n",
      "Epoch 88/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9833 - metric_VAMP: 2.9916 - val_loss: -2.9628 - val_metric_VAMP: 2.9813\n",
      "Epoch 89/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9839 - metric_VAMP: 2.9919 - val_loss: -2.9627 - val_metric_VAMP: 2.9813\n",
      "Epoch 90/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9839 - metric_VAMP: 2.9920 - val_loss: -2.9632 - val_metric_VAMP: 2.9815\n",
      "Epoch 91/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9843 - metric_VAMP: 2.9921 - val_loss: -2.9634 - val_metric_VAMP: 2.9816\n",
      "Epoch 92/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9844 - metric_VAMP: 2.9922 - val_loss: -2.9632 - val_metric_VAMP: 2.9815\n",
      "Epoch 93/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9843 - metric_VAMP: 2.9921 - val_loss: -2.9632 - val_metric_VAMP: 2.9815\n",
      "Epoch 94/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9844 - metric_VAMP: 2.9922 - val_loss: -2.9630 - val_metric_VAMP: 2.9814\n",
      "Epoch 95/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9847 - metric_VAMP: 2.9924 - val_loss: -2.9622 - val_metric_VAMP: 2.9810\n",
      "Epoch 96/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9849 - metric_VAMP: 2.9925 - val_loss: -2.9629 - val_metric_VAMP: 2.9814\n",
      "Epoch 97/100\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -2.9850 - metric_VAMP: 2.9925 - val_loss: -2.9624 - val_metric_VAMP: 2.9812\n",
      "Setting initial auxiliary weights...\n",
      "Training auxiliary network...\n",
      "Both valid loss: -3.9165751934051514\n",
      "Training full network...\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 10s 22us/sample - loss: -3.9475 - val_loss: -3.9330\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9555 - val_loss: -3.9366\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9595 - val_loss: -3.9393\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9622 - val_loss: -3.9426\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9642 - val_loss: -3.9444\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9658 - val_loss: -3.9457\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9670 - val_loss: -3.9468\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9682 - val_loss: -3.9478\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9690 - val_loss: -3.9478\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9697 - val_loss: -3.9484\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9704 - val_loss: -3.9491\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9710 - val_loss: -3.9489\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9716 - val_loss: -3.9496\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9722 - val_loss: -3.9497\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9727 - val_loss: -3.9500\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9731 - val_loss: -3.9504\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9734 - val_loss: -3.9503\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9737 - val_loss: -3.9507\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9741 - val_loss: -3.9509\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9744 - val_loss: -3.9505\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9747 - val_loss: -3.9510\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9749 - val_loss: -3.9506\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9752 - val_loss: -3.9503\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9756 - val_loss: -3.9509\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9759 - val_loss: -3.9516\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9761 - val_loss: -3.9510\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9763 - val_loss: -3.9510\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9765 - val_loss: -3.9508\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9768 - val_loss: -3.9513\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9770 - val_loss: -3.9514\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9771 - val_loss: -3.9517\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9773 - val_loss: -3.9515\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9774 - val_loss: -3.9513\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9778 - val_loss: -3.9511\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9780 - val_loss: -3.9513\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9782 - val_loss: -3.9514\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9783 - val_loss: -3.9519\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9785 - val_loss: -3.9509\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9785 - val_loss: -3.9513\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9786 - val_loss: -3.9517\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9789 - val_loss: -3.9509\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9790 - val_loss: -3.9513\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9792 - val_loss: -3.9518\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9793 - val_loss: -3.9512\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9794 - val_loss: -3.9514\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9797 - val_loss: -3.9514\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9797 - val_loss: -3.9515\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9799 - val_loss: -3.9514\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9799 - val_loss: -3.9511\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9799 - val_loss: -3.9509\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9801 - val_loss: -3.9511\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9802 - val_loss: -3.9507\n",
      "Epoch 53/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9802 - val_loss: -3.9511\n",
      "Epoch 54/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9805 - val_loss: -3.9507\n",
      "Epoch 55/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9806 - val_loss: -3.9506\n",
      "Epoch 56/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9806 - val_loss: -3.9507\n",
      "Epoch 57/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9807 - val_loss: -3.9506\n",
      "Epoch 58/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9808 - val_loss: -3.9509\n",
      "Epoch 59/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9810 - val_loss: -3.9511\n",
      "Epoch 60/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9811 - val_loss: -3.9505\n",
      "Epoch 61/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9813 - val_loss: -3.9507\n",
      "Epoch 62/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9814 - val_loss: -3.9501\n",
      "Epoch 63/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9816 - val_loss: -3.9509\n",
      "Epoch 64/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9815 - val_loss: -3.9508\n",
      "Epoch 65/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9816 - val_loss: -3.9502\n",
      "Epoch 66/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9815 - val_loss: -3.9505\n",
      "Epoch 67/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9818 - val_loss: -3.9503\n",
      "Epoch 68/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9819 - val_loss: -3.9501\n",
      "Epoch 69/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9819 - val_loss: -3.9501\n",
      "Epoch 70/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9817 - val_loss: -3.9497\n",
      "Epoch 71/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9821 - val_loss: -3.9506\n",
      "Epoch 72/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9821 - val_loss: -3.9500\n",
      "Epoch 73/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9823 - val_loss: -3.9504\n",
      "Epoch 74/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9822 - val_loss: -3.9497\n",
      "Epoch 75/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9825 - val_loss: -3.9501\n",
      "Epoch 76/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9825 - val_loss: -3.9504\n",
      "Epoch 77/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9827 - val_loss: -3.9507\n",
      "Epoch 78/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9826 - val_loss: -3.9494\n",
      "Epoch 79/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9826 - val_loss: -3.9495\n",
      "Epoch 80/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9826 - val_loss: -3.9499\n",
      "Epoch 81/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9826 - val_loss: -3.9493\n",
      "Epoch 82/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9828 - val_loss: -3.9496\n",
      "Epoch 83/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9828 - val_loss: -3.9498\n",
      "Epoch 84/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9830 - val_loss: -3.9495\n",
      "Epoch 85/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9831 - val_loss: -3.9497\n",
      "Epoch 86/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9830 - val_loss: -3.9494\n",
      "Epoch 87/10000\n",
      "450000/450000 [==============================] - 3s 7us/sample - loss: -3.9832 - val_loss: -3.9493\n",
      "Setting up auxiliary training loop...\n",
      "Both valid loss: -3.9165751934051514\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 9s 21us/sample - loss: -3.9784 - val_loss: -3.9523\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9789 - val_loss: -3.9532\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9790 - val_loss: -3.9526\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9791 - val_loss: -3.9532\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9793 - val_loss: -3.9523\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9795 - val_loss: -3.9518\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9796 - val_loss: -3.9526\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9798 - val_loss: -3.9531\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9799 - val_loss: -3.9524\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9801 - val_loss: -3.9518\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9802 - val_loss: -3.9522\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9802 - val_loss: -3.9525\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9803 - val_loss: -3.9525\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9805 - val_loss: -3.9525\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9805 - val_loss: -3.9516\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9806 - val_loss: -3.9524\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9807 - val_loss: -3.9523\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9808 - val_loss: -3.9519\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9810 - val_loss: -3.9519\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9811 - val_loss: -3.9520\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9812 - val_loss: -3.9516\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9812 - val_loss: -3.9522\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9814 - val_loss: -3.9519\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9815 - val_loss: -3.9513\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9815 - val_loss: -3.9528\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9817 - val_loss: -3.9514\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9818 - val_loss: -3.9518\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9819 - val_loss: -3.9517\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9819 - val_loss: -3.9518\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9820 - val_loss: -3.9516\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9821 - val_loss: -3.9512\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9822 - val_loss: -3.9513\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9823 - val_loss: -3.9512\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9824 - val_loss: -3.9517\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9825 - val_loss: -3.9512\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9825 - val_loss: -3.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9826 - val_loss: -3.9517\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9826 - val_loss: -3.9508\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9828 - val_loss: -3.9511\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9828 - val_loss: -3.9515\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9829 - val_loss: -3.9508\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9829 - val_loss: -3.9508\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9829 - val_loss: -3.9505\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9831 - val_loss: -3.9507\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9832 - val_loss: -3.9506\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9832 - val_loss: -3.9505\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9832 - val_loss: -3.9508\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9833 - val_loss: -3.9503\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9833 - val_loss: -3.9511\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9833 - val_loss: -3.9499\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9835 - val_loss: -3.9502\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9836 - val_loss: -3.9504\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: -3.9532\n",
      "Iteration 0: Score: 3.9531580924987795\n",
      "Both valid loss: -3.9165751934051514\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/10000\n",
      "450000/450000 [==============================] - 3s 6us/sample - loss: -3.9791 - val_loss: -3.9527\n",
      "Epoch 2/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9793 - val_loss: -3.9527\n",
      "Epoch 3/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9794 - val_loss: -3.9529\n",
      "Epoch 4/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9796 - val_loss: -3.9526\n",
      "Epoch 5/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9797 - val_loss: -3.9513\n",
      "Epoch 6/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9799 - val_loss: -3.9527\n",
      "Epoch 7/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9800 - val_loss: -3.9531\n",
      "Epoch 8/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9800 - val_loss: -3.9528\n",
      "Epoch 9/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9802 - val_loss: -3.9528\n",
      "Epoch 10/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9803 - val_loss: -3.9521\n",
      "Epoch 11/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9805 - val_loss: -3.9519\n",
      "Epoch 12/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9806 - val_loss: -3.9521\n",
      "Epoch 13/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9807 - val_loss: -3.9528\n",
      "Epoch 14/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9808 - val_loss: -3.9523\n",
      "Epoch 15/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9810 - val_loss: -3.9520\n",
      "Epoch 16/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9810 - val_loss: -3.9520\n",
      "Epoch 17/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9812 - val_loss: -3.9515\n",
      "Epoch 18/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9810 - val_loss: -3.9519\n",
      "Epoch 19/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9812 - val_loss: -3.9515\n",
      "Epoch 20/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9812 - val_loss: -3.9514\n",
      "Epoch 21/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9814 - val_loss: -3.9517\n",
      "Epoch 22/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9815 - val_loss: -3.9520\n",
      "Epoch 23/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9817 - val_loss: -3.9517\n",
      "Epoch 24/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9818 - val_loss: -3.9524\n",
      "Epoch 25/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9817 - val_loss: -3.9509\n",
      "Epoch 26/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9818 - val_loss: -3.9515\n",
      "Epoch 27/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9820 - val_loss: -3.9510\n",
      "Epoch 28/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9820 - val_loss: -3.9517\n",
      "Epoch 29/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9822 - val_loss: -3.9515\n",
      "Epoch 30/10000\n",
      "450000/450000 [==============================] - 2s 6us/sample - loss: -3.9823 - val_loss: -3.9514\n",
      "Epoch 31/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9823 - val_loss: -3.9516\n",
      "Epoch 32/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9824 - val_loss: -3.9511\n",
      "Epoch 33/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9825 - val_loss: -3.9518\n",
      "Epoch 34/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9826 - val_loss: -3.9510\n",
      "Epoch 35/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9826 - val_loss: -3.9514\n",
      "Epoch 36/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9828 - val_loss: -3.9505\n",
      "Epoch 37/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9828 - val_loss: -3.9513\n",
      "Epoch 38/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9829 - val_loss: -3.9507\n",
      "Epoch 39/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9831 - val_loss: -3.9512\n",
      "Epoch 40/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9831 - val_loss: -3.9509\n",
      "Epoch 41/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9831 - val_loss: -3.9498\n",
      "Epoch 42/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9832 - val_loss: -3.9506\n",
      "Epoch 43/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9832 - val_loss: -3.9503\n",
      "Epoch 44/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9832 - val_loss: -3.9502\n",
      "Epoch 45/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9834 - val_loss: -3.9514\n",
      "Epoch 46/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9834 - val_loss: -3.9501\n",
      "Epoch 47/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9834 - val_loss: -3.9504\n",
      "Epoch 48/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9835 - val_loss: -3.9501\n",
      "Epoch 49/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9836 - val_loss: -3.9504\n",
      "Epoch 50/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9838 - val_loss: -3.9503\n",
      "Epoch 51/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9838 - val_loss: -3.9496\n",
      "Epoch 52/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9839 - val_loss: -3.9500\n",
      "Epoch 53/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9839 - val_loss: -3.9496\n",
      "Epoch 54/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9840 - val_loss: -3.9497\n",
      "Epoch 55/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9840 - val_loss: -3.9502\n",
      "Epoch 56/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9842 - val_loss: -3.9503\n",
      "Epoch 57/10000\n",
      "450000/450000 [==============================] - 2s 5us/sample - loss: -3.9842 - val_loss: -3.9492\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: -3.9531\n",
      "Iteration 1: Score: 3.9530738830566405\n",
      "Performing final fit...\n",
      "Both valid loss: -3.9165751934051514\n",
      "Lag loss: -3.987867832183838\n",
      "Lag loss: -3.987867832183838\n",
      "Lag loss: -3.987867832183838\n",
      "Lag loss: -3.987867832183838\n",
      "Lag loss: -3.987867832183838\n",
      "Lag loss: -3.987867832183838\n",
      "Lag loss: -3.987867832183838\n",
      "Lag loss: -3.987867832183838\n",
      "118428/118428 [==============================] - 1s 4us/sample - loss: -3.9141\n",
      "EVAL COMPLETE\n",
      "=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~\n",
      "Scores:\n",
      "  3.789445\n",
      "  3.894452\n",
      "  3.914105\n",
      "\n",
      "=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 299,
       "width": 327
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k, n = \"holo\", 4\n",
    "history = [tuple(to_number(val) for val in f.stem.split(\"_\")[1:])\n",
    "           for f in Path(\"hp-figs\").iterdir()]\n",
    "for _ in range(n_iter):\n",
    "    vals = OrderedDict((k, random.choice(v)) for k, v in global_space.items())\n",
    "    if tuple(vals.values()) in history:\n",
    "        continue\n",
    "    history.append(tuple(vals.values()))\n",
    "    print(\"EVAL BEGIN\")\n",
    "    print(\"=~\" * 40)\n",
    "    print(\"Parameters:\")\n",
    "    for key, val in vals.items():\n",
    "        print(\"  {0}: \\t\\t{1}\".format(key, val))\n",
    "    print()\n",
    "    its, scores = test_model(vals)\n",
    "    \n",
    "    print(\"EVAL COMPLETE\")\n",
    "    print(\"=~\" * 40)\n",
    "    print(\"Scores:\")\n",
    "    for score in scores:\n",
    "        print(\"  {0:4.6f}\".format(score))\n",
    "    print()\n",
    "    print(\"=~\" * 40)\n",
    "    \n",
    "    fig = plot_its(its, lags)\n",
    "    fig.axes[0].text(30, 3, \"{0:4.6f}\".format(np.nanmean(scores)), fontsize=24)\n",
    "    fig.savefig(\"hp-figs/hp-{0}.png\".format(\"_\".join(str(v) for v in vals.values())),\n",
    "                bbox_inches=\"tight\", transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
